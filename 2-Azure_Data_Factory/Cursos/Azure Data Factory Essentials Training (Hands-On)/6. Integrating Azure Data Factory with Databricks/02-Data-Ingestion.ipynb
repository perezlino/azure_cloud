{"cells":[{"cell_type":"markdown","source":["# Data ingestion via Azure Data Factory\n\nIn this notebook, you will create an Azure Data Factory (ADF) v2 pipeline to ingest data from a public dataset into your Azure Storage account.\n\nIn this lesson you will complete the following:\n* Create a copy data pipeline.\n* Monitor the pipeline run.\n* Verify copied files exist in Blob storage.\n* Examine the ingested data."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a57a9640-a2cd-4db7-8b39-921f98f696f4","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Create a copy data pipeline\n\nWe will start the lab by using the Copy Data Wizard to create a new ADF pipeline using the Azure Data Factory UI. The wizard handles creating Linked Services, Data Sources, and the Copy Activity for you.\n\nTo learn more about the various components of ADF, you can visit the following resources:\n\n* [Pipeline and actvities](https://docs.microsoft.com/en-us/azure/data-factory/concepts-pipelines-activities)\n* [Dataset and linked services](https://docs.microsoft.com/en-us/azure/data-factory/concepts-datasets-linked-services)\n* [Pipeline execution and triggers](https://docs.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers)\n* [Integration runtime](https://docs.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime)\n\nIn the [Azure portal](https://portal.azure.com/), navigate to the ADF instance you provisioned in the Getting Started notebook, and then launch the Azure Data Factory UI by select the **Author & Monitor** tile on the ADF Overview blade.\n\n![Author & Monitor](https://databricksdemostore.blob.core.windows.net/images/03/02/adf-author-and-deploy.png \"Author & Monitor\")\n\nOn the ADF UI landing page, select **Copy data**.\n\n![Copy data](https://databricksdemostore.blob.core.windows.net/images/03/02/adf-copy-data.png \"Copy pipeline\")\n\n### Step 1: Properties\n\nOn the Properties page of the Copy Data wizard, do the following:\n\n1. **Task name**: Enter a name, such as LabPipeline.\n2. **Task cadence or Task schedule**: Choose Run once now.\n3. Select **Next**.\n\n![Copy Data wizard properties](https://databricksdemostore.blob.core.windows.net/images/03/02/adf-copy-data-step-1.png \"Copy Data wizard properties\")\n\n### Step 2: Source\n\nThe source will be configured to point to a publicly accessible Azure Storage account, which contains the files you will be copying into your own storage account.\n\nOn the Source data store page, select **+ Create new connection**.\n\n![Create new connection](https://databricksdemostore.blob.core.windows.net/images/03/02/adf-create-new-connection.png \"Create new connection\")\n\nOn the New Linked Service blade:\n\n1. Enter \"storage\" into the search box.\n2. Select **Azure Blob Storage**.\n3. Select **Continue**.\n\n![Add Azure Blob Storage Linked Service](https://databricksdemostore.blob.core.windows.net/images/03/02/adf-linked-service-azure-blob-storage.png \"Add Azure Blob Storage Linked Service\")\n\nConfigure the New Linked Service (Azure Blob Storage) with the following values:\n\n1. **Name**: Enter PublicDataset\n2. **Authentication method**: Select Use SAS URI\n3. **SAS URI**: Paste the following URI into the field: <https://databricksdemostore.blob.core.windows.net/?sv=2017-11-09&ss=b&srt=sco&sp=rl&se=2099-12-31T17:59:59Z&st=2018-09-22T15:21:51Z&spr=https&sig=LqDcqVNGNEuWILjNJoThzaXktV2N%2BFS354s716RJo80%3D>\n\n4. Select **Test connection** and ensure a Connection successful message is displayed\n5. Select **Finish**\n\n![Configure Azure Blob Storage Linked Service for public dataset](https://databricksdemostore.blob.core.windows.net/images/03/02/adf-linked-service-public-dataset.png \"Configure Azure Blob Storage Linked Service for public dataset\")\n\nBack on the Source data source page, ensure **PublicDataset** is selected, and then select **Next**.\n\n![Source data store](https://databricksdemostore.blob.core.windows.net/images/03/02/adf-source-data-store.png \"Source data store\")\n\nOn the Choose the input file or folder page:\n\n1. **File or folder**: Use the **Browse** button to select the folder **training/crime-data-2016/**. *Do not enter the path as text, you must use the browse button due to a bug in some versions of the interface*\n2. **Copy file recursively**: Check this box.\n3. **Binary Copy**: Check this box.\n4. Select **Next**.\n\n![Choose the input file or folder](https://databricksdemostore.blob.core.windows.net/images/03/02/adf-choose-input-file-or-folder.png \"Choose the input file or folder\")\n\n### Step 3: Destination\n\nThe destination data store will be configured to point to the Azure Storage account you created in the [Getting Started notebook]($./01-Getting-Started.dbc) within this lab.\n\nOn the Destination data store page, select **+ Create new connection**.\n\n![Create new connection](https://databricksdemostore.blob.core.windows.net/images/03/02/adf-create-new-connection.png \"Create new connection\")\n\nAs you did previously, enter \"storage\" into the search box on the New Linked Service blade, choose **Azure Blob Storage** for the Linked Service and select **Continue**.\n\nConfigure the New Linked Service (Azure Blob Storage) with the following values:\n\n1. **Name**: Enter DestinationContainer.\n2. **Authentication method**: Select Use account key.\n3. **Storage account name**: Select the name of the Storage account you created in the Getting Started notebook from the list.\n4. Select **Test connection** and ensure a Connection successful message is displayed.\n5. Select **Finish**.\n\n![Configure Azure Blob Storage Linked Service for destination container](https://databricksdemostore.blob.core.windows.net/images/03/02/adf-linked-service-destination-container.png \"Configure Azure Blob Storage Linked Service for destination container\")\n\nBack on the Destination data store view, ensure **DestinationContainer** is selected, and then select **Next**.\n\nOn the Choose the output file or folder page:\n\n1. **Folder path**: Enter **dwtemp/03.02/**.\n2. **File name**: Leave empty.\n3. **Copy behavior**: Select **Preserve hierarchy**.\n4. Select **Next**.\n\n![Choose the output file or folder](https://databricksdemostore.blob.core.windows.net/images/03/02/adf-choose-output-file-or-folder.png \"Choose the output file or folder\")\n\n### Step 4: Settings\n\nOn the Settings page, accept the default values, and select **Next**.\n\n### Step 5: Summary\n\nOn the Summary page, you can review the copy pipeline settings, and then select **Next** to deploy the pipeline.\n\n### Step 6: Deployment\n\nThe Deployment page displays the status of the pipeline deployment. This will create the Datasets and Pipeline, and then run the pipeline. Select **Monitor** to view the pipeline progress.\n\n  ![Pipeline Deployment](https://databricksdemostore.blob.core.windows.net/images/03/02/adf-deployment.png \"Pipeline Deployment\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"43955e8b-c0e6-454b-b587-bd826eda9eb6","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Monitor the pipeline run\n\nSelecting Monitor above will take you to the Pipeline Runs screen in the ADF UI, where you can monitor the status of your pipeline runs. Using the monitor dialog, you can track the completion status of pipeline runs, and access other details about the run.\n\n  ![Monitor pipeline runs](https://databricksdemostore.blob.core.windows.net/images/03/02/adf-monitor-pipeline-runs.png \"Monitor pipeline runs\")\n  \n> NOTE: You may need to select Refresh if you don't see the pipeline listed, or to update the status displayed.\n\nYou can select the View Activity Runs icon under Actions if you want to see the progress if the individual activities that make up the pipeline. On the Activity Runs dialog, you can select the various icons under Actions to display the inputs, outputs, and run details.\n\n  ![Monitor activity runs](https://databricksdemostore.blob.core.windows.net/images/03/02/adf-monitor-activity-runs.png \"Monitor activity runs\")\n\nWhen the copy activity is completed, its status will change to Succeeded (requires refreshing the activities list)."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6a418a4c-1023-4f14-aba8-68b64c45b0ad","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Verify files in blob storage\n\nNow, navigate to your storage account in the Azure portal, and locate the `dwtemp` container, and the `03.02` folder within it. Observe the files copied via ADF.\n\n  ![Copied files in Blob storage](https://databricksdemostore.blob.core.windows.net/images/03/02/blob-storage-files.png \"Blob storage files\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"43af6c57-07e3-4d42-b34c-4d95bee73716","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["-sandbox\n## Examine the ingested data\n\nQuickly examine a few of the crime datasets ingested by the above operation, and observe some of the differences in the datasets from each city.\n\nFirst, run the cell below to create a direct connection to your Blob Storage account, replacing the values of `storageAccountName` and `storageAccountKey` with the appropriate values from your storage account. You retrieved these values in the [Getting Started notebook]($./01-Getting-Started).\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/>Remember to attach your notebook to a cluster before running any cells in your notebook. In the notebook's toolbar, select the drop down arrow next to Detached, then select your cluster under Attach to.\n\n![Attached to cluster](https://databricksdemostore.blob.core.windows.net/images/03/03/databricks-cluster-attach.png)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fe10ebfb-6309-4850-baf1-4768462602cb","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["containerName = \"dwtemp\"\nstorageAccountName = \"[your-storage-account-name]\"\nstorageAccountKey = \"[your-storage-account-key]\"\n\nspark.conf.set(\n  \"fs.azure.account.key.%(storageAccountName)s.blob.core.windows.net\" % locals(),\n  storageAccountKey)\n\nconnectionString = \"wasbs://%(containerName)s@%(storageAccountName)s.blob.core.windows.net/03.02\" % locals()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"20856eb8-1e94-4183-9bdf-09905d06b922","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Start by reviewing the `crime-data-2016` parquet files copied from the public storage account.\n\nIn the below cell, replace `[your-storage-account-name]` with the name of your storage account and then run the cell. This will list the parquet files, containing crime data for Boston, New York, and other cities."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b73f4bbc-a262-4ceb-b15b-c20619c3ae95","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%fs ls wasbs://dwtemp@[your-storage-account-name].blob.core.windows.net/03.02"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7e8510e3-b4a4-4d35-b4e1-e16d2f319c74","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The next step is to examine the data for a few cities closer by creating a DataFrame for each file.\n\nStart by creating a DataFrame for the New York and Boston data."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"371d75b9-a251-4258-bf17-6406cf0d50a6","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["crimeDataNewYorkDf = spark.read.parquet(\"%(connectionString)s/Crime-Data-New-York-2016.parquet\" % locals())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c52aa117-b7dd-45d9-8943-d0250ea4fbf4","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["crimeDataBostonDf = spark.read.load(\"%(connectionString)s/Crime-Data-Boston-2016.parquet\" % locals())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a0a8f91c-53db-4a9e-80b2-3723a8913a75","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["With the two DataFrames created, it is now possible to review the first couple records of each file."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4d93d3a6-524f-4bc6-b175-bc5a0802c9fc","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["display(crimeDataNewYorkDf)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b0f4fbf0-8916-43e4-bcd1-dc6df8807e24","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(crimeDataBostonDf)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"977ed6ec-33ef-40a0-9d74-00430e944f1a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Same Type of Data, Different Structure\n\nNotice in the examples above:\n* The `crimeDataNewYorkDF` and `crimeDataBostonDF` DataFrames use different names for the columns.\n* The data itself is formatted differently and different names are used for similar concepts.\n\nThis is common when pulling data from disparate data sources.\n\nIn the next lesson, we will use an ADF Databricks Notebooks activity to perform data cleanup and extract homicide statistics."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"34312051-caf5-4d73-9908-27edbcd0a213","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Next Steps\n\nStart the next lesson, [Data Transformation]($./03-Data-Transformation)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e3d46010-7193-4925-92ab-82afd74e7f91","inputWidgets":{},"title":""}}}],"metadata":{"celltoolbar":"Raw Cell Format","kernelspec":{"display_name":"PySpark3","language":"","name":"pyspark3kernel"},"language_info":{"codemirror_mode":{"name":"python","version":"3"},"mimetype":"text/x-python","name":"pyspark3","pygments_lexer":"python3"},"application/vnd.databricks.v1+notebook":{"notebookName":"02-Data-Ingestion","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
