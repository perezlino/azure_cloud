{"cells":[{"cell_type":"markdown","source":["# Getting Started\n\nIn this lab you will learn how Azure Data Factory (ADF) v2 can be used to orchestrate the ingestion and transformation of data with Azure Databricks. You will use a Copy activity to copy a public dataset into a location accessible to your Azure Databricks cluster. Then, a Databricks notebook activity will be employed to batch process the files within that dataset, identify issues with the data, clean and format the data, and load it into Databricks global tables to support downstream analytics.\n\nThis notebook provides the steps required to provision the necessary pre-requisite resources in your Azure subscription to complete this lab.\n\n## Pre-Requisites\n\nPrior to beginning this lab, there are a few resources you need to provision within the Azure subscription you are using for this lab.\n\n1. Gneral purpose Azure Storage account\n2. Azure Data Factory v2"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a57a9640-a2cd-4db7-8b39-921f98f696f4","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Azure Storage \n\nFor this lab, Azure Storage blobs will be used as the intermediary for the exchange of data between ADF and Azure Databricks. To facilitate this, you will need to:\n\n1. Create a general purpose Azure Storage account v1\n2. Acquire the Account Name and Account Key for that Storage Account \n3. Create a container named `dwtemp` that will be used to store data used during the exchange.\n\n#### Create Azure Storage account\n\n1. In the [Azure portal](https://portal.azure.com), select **+ Create a resource**, enter \"storage account\" into the Search the Marketplace box, select **Storage account - blob, file, table, queue** from the results, and select **Create**.\n\n   ![In the Azure portal, +Create a resource is highlighted in the navigation pane, \"storage account\" is entered into the Search the Marketplace box, and Storage account - blob, file, table, queue is highlighted in the results.](https://databricksdemostore.blob.core.windows.net/images/02-SQL-DW/create-resource-storage-account.png 'Create Storage account')\n\n2. In the Create storage account blade, enter the following:\n\n   - **Subscription**: Select the subscription you are using for this module.\n   - **Resource group**: Choose your module resource group.\n   - **Storage account name**: Enter a unique name (make sure you see a green checkbox).\n   - **Location**: Select the location you are using for resources in this module.\n   - **Performance**: Select Standard.\n   - **Account kind**: Select Storage (general purpose v1).\n   - **Replication**: Select Locally-redundant storage (LRS).\n\n   ![The Create storage account blade is displayed, with the previously mentioned settings entered into the appropriate fields.](https://databricksdemostore.blob.core.windows.net/images/02-SQL-DW/storage-account-create-new.png 'Create storage account')\n\n3. Select **Next: Advanced >**.\n\n4. In the Advanced tab, select the following:\n\n   - **Secure transfer required**: Select Disabled\n   - **Virtual network**: Select None\n\n   ![The Create storage account blade is displayed with options under the Advanced tab.](https://databricksdemostore.blob.core.windows.net/images/02-SQL-DW/storage-account-create-new-advanced.png 'Create storage account - Advanced')\n\n5. Select **Review + create**.\n\n6. In the Review tab, select **Create**.\n\n#### Acquire account name and key\n\nOnce provisioned, navigate to your storage account, select **Access keys** from the left-hand menu, and copy the **Storage account name** and **key1** Key value into a text editor, such as Notepad, for later use.\n\n   ![Copy the storage account name and key1 from the Keys blade.](https://databricksdemostore.blob.core.windows.net/images/02-SQL-DW/storage-account-keys.png 'Storage account keys')\n   \n#### Create the dwtemp container\n\nSelect **Blobs** from the left-hand menu, then select **+ container** to create a new container. Enter `dwtemp` for the container name, leave the public access level selected as Private, then select **OK**.\n\n   ![Create new container.](https://databricksdemostore.blob.core.windows.net/images/02-SQL-DW/storage-account-create-container.png 'New container')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"21b694c3-70ba-43f4-a404-4a40b151d419","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Azure Data Factory (ADF) v2\n[ADF v2](https://docs.microsoft.com/en-us/azure/data-factory/) will be used to orchestrate the ingestion and transformation of data in this lab. For this, you will need to:\n\n1. Create an ADF v2 instance in your Azure account by opening a new browser window or tab and navigating to the [Azure Portal](https://portal.azure.com) (https://portal.azure.com).\n\n2. Once there, select **+ Create a resource**, type \"data factory\" into the search bar, select **Data Factory** in the results, and then select **Create**.\n\n  ![Select create a resource, type in Data Factory, then select Data Factory from the results list](https://databricksdemostore.blob.core.windows.net/images/03/01/create-resource-adf.png \"Create Data Factory\")\n\n3.  Set the following configuration on the Data Factory creation form:\n\n    - **Name**: Enter a globally unique name, as indicated by a green checkmark.\n    - **Subscription**: Select the subscription you are using for this workshop.\n    - **Resource Group**: Choose Use existing, and select the resource group for this workshop.\n    - **Version**: V2\n    - **Location**: Select a region.\n    \n    ![Complete the Azure Data Factory creation form with the options as outlined above.](https://databricksdemostore.blob.core.windows.net/images/03/01/adf-create-new.png \"Create New Data Factory\")\n\n4.  Select **Create** to provision ADF v2."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fc458762-16bb-4c67-b12b-ce436ef6d2da","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Next steps\n\nStart the next lesson, [Data Ingestion]($./02-Data-Ingestion)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e3d46010-7193-4925-92ab-82afd74e7f91","inputWidgets":{},"title":""}}}],"metadata":{"celltoolbar":"Raw Cell Format","kernelspec":{"display_name":"PySpark3","language":"","name":"pyspark3kernel"},"language_info":{"codemirror_mode":{"name":"python","version":"3"},"mimetype":"text/x-python","name":"pyspark3","pygments_lexer":"python3"},"application/vnd.databricks.v1+notebook":{"notebookName":"01-Getting-Started","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
