{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio - Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "Se requiere que obtenga los datos de la \"Categoria de los productos\" y los \"Productos\", crear una columna 'Total' que contiene \n",
    "el \"PrecioUnitario*Cantidad\".\n",
    "\n",
    "Luego de eso mediante la transformación \"Window\" crear las Funciones de ventana \"SUM, AVG, COUNT, MAX y MIN\". Finalmente la \n",
    "data tiene que estar en un archivo '.txt' de salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "          SOURCE                           WINDOW                         SINK                       \n",
    " _________________________          ___________________          _______________________\n",
    "|                         |        |                   |        |                       |                    \n",
    "| sourceProductoCategoria |--------|       Window      |--------| sinkProductoCategoria |\n",
    "|_________________________| +      |___________________| +      |_______________________| "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sourceProductoCategoria\n",
    "\n",
    "- Utilizamos \"Inline Dataset\" (obtenemos el mismo resultado que utilizando \"Dataset\", no se cual es la real diferencia)\n",
    "- Inline Dataset Type: escogemos el tipo de origen\n",
    "- Linked service: escogemos el Linked Service de origen, y este apunta hacia Azure SQL Database, a una base de datos especifica\n",
    "- Debug mode: al activarlo nos permitirá acceder a las opciones de Projection y Data preview. También nos permitirá visualizar\n",
    "              las tablas en Source options\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/jdmJkjpN/adf139.png\"></center>\n",
    "\n",
    "- Pegamos la Query\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/qvQgHMYj/adf140.png\"></center>\n",
    "\n",
    "- Importamos el schema\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/yYy808Vs/adf141.png\"></center>\n",
    "\n",
    "\n",
    "##### Window\n",
    "\n",
    "- La función \"toDecimal\" nos devuelve solo 2 números decimales\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/K4S3jqrf/adf142.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/Y0wmqDx3/adf143.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/mgr1DH70/adf144.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/pTsgP3xN/adf145.png\"></center>\n",
    "\n",
    "\n",
    "##### sinkProductoCategoria\n",
    "\n",
    "- Utilizamos \"Inline Dataset\" (obtenemos el mismo resultado que utilizando \"Dataset\", no se cual es la real diferencia)\n",
    "- Inline Dataset Type: escogemos el tipo de archivo de salida\n",
    "- Linked service: escogemos el Linked Service de destino, y este apunta hacia Azure Blob Storage, a una ruta especifica\n",
    "- Debug mode: al activarlo nos permitirá acceder a las opciones de Mapping y Data preview.\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/fyXpYyms/adf146.png\"></center>\n",
    "\n",
    "- El directorio \"14.Window\" se creará de manera automática dentro del contenedor \"dataflowdataset\"\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/dQ85RzgJ/adf147.png\"></center>\n",
    "\n",
    "- No particionamos el archivo y se creará un único archivo llamado \"ProductoCategoria.csv\". Debemos pulsar sobre\n",
    "  el botón de \"Set single partition\" que aparecerá\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/W37Jz6fc/adf148.png\"></center>\n",
    "\n",
    "\n",
    "##### Ejecución del Pipeline y resultados\n",
    "\n",
    "- Despues de haber finalizado nuestro Data Flow, lo ejecutaremos dentro de una actividad \"Data Flow\" desde un Pipeline\n",
    "- Podemos ver que se ha creado el archivo\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/Y2nhPS5R/adf149.png\"></center>  \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
