{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio - Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "Se requiere unir varios archivos delimitados por coma (CSV). Dichos archivos contienen información de películas y un archivo \n",
    "esta completamente estructurado, el otro no tiene encabezado y finalmente el último tiene las columnas desordenadas.\n",
    "\n",
    "Mediante Azure Data Factory con el componente \"Union\" se requiere unir dichos archivos y generar un único archivo de salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "- Union1: Corresponde a la \"Union\" entre los origenes 'SourceMovie1' y 'SourceMovie2'\n",
    "- Union2: Corresponde a la \"Union\" entre el resultado obtenido en 'Union1' y el origen 'SourceMovie3'  \n",
    "\n",
    "      SOURCE                      UNION                       UNION                         SINK\n",
    " ________________          ___________________          ___________________          ___________________        \n",
    "|                |        |                   |        |                   |        |                   |      \n",
    "|  sourceMovie1  |--------|      Union1       |--------|      Union2       |--------|     sinkMovie     |\n",
    "|________________| +      |___________________| +      |___________________| +      |___________________|\n",
    "\n",
    "      SOURCE                      \n",
    " ________________               \n",
    "|                |            \n",
    "|  sourceMovie2  |\n",
    "|________________| \n",
    "                                         \n",
    "      SOURCE                    \n",
    " ________________               \n",
    "|                |                \n",
    "|  sourceMovie3  |\n",
    "|________________| "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sourceMovie1\n",
    "\n",
    "- Utilizamos \"Inline Dataset\" (obtenemos el mismo resultado que utilizando \"Dataset\", no se cual es la real diferencia)\n",
    "- Inline Dataset Type: escogemos el tipo de archivo de origen\n",
    "- Linked service: escogemos el Linked Service de origen, y este apunta hacia Azure Blob Storage, a una ruta especifica\n",
    "- Debug mode: al activarlo nos permitirá acceder a las opciones de Projection y Data preview\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/s270TmSY/adf60.png\"></center>\n",
    "\n",
    "- También activamos la casilla de \"First Row Header\"\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/1t4JNZP3/adf61.png\"></center>\n",
    "\n",
    "\n",
    "##### sourceMovie2\n",
    "\n",
    "- Utilizamos \"Inline Dataset\" (obtenemos el mismo resultado que utilizando \"Dataset\", no se cual es la real diferencia)\n",
    "- Inline Dataset Type: escogemos el tipo de archivo de origen\n",
    "- Linked service: escogemos el Linked Service de origen, y este apunta hacia Azure Blob Storage, a una ruta especifica\n",
    "- Debug mode: al activarlo nos permitirá acceder a las opciones de Projection y Data preview\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/PrMYhRs3/adf62.png\"></center>\n",
    "\n",
    "- En esta ocasión NO activamos la casilla de \"First Row Header\" dado que este archivo \"DataSetMovie2.csv\" NO TIENE ENCABEZADOS\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/nz8Cq4f9/adf63.png\"></center>\n",
    "\n",
    "\n",
    "##### Union1\n",
    "\n",
    "- Escogemos \"Union by: Position\" dado que el archivo \"DataSetMovie2.csv\" no tiene encabezados, los archivos los uniremos por\n",
    "  posición.\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/4xB0KPtw/adf64.png\"></center>\n",
    "\n",
    "\n",
    "##### sourceMovie3\n",
    "\n",
    "- Utilizamos \"Inline Dataset\" (obtenemos el mismo resultado que utilizando \"Dataset\", no se cual es la real diferencia)\n",
    "- Inline Dataset Type: escogemos el tipo de archivo de origen\n",
    "- Linked service: escogemos el Linked Service de origen, y este apunta hacia Azure Blob Storage, a una ruta especifica\n",
    "- Debug mode: al activarlo nos permitirá acceder a las opciones de Projection y Data preview\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/1Xm6NGDc/adf65.png\"></center>\n",
    "\n",
    "- También activamos la casilla de \"First Row Header\"\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/6Qx8vVX2/adf66.png\"></center>\n",
    "\n",
    "\n",
    "##### Union2\n",
    "\n",
    "- Escogemos \"Union by: Name\" dado que el archivo \"DataSetMovie3.csv\" si tiene encabezados, los archivos los uniremos por\n",
    "  nombre de columna.\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/wB5QyTSz/adf67.png\"></center>\n",
    "\n",
    "\n",
    "##### sinkMovie\n",
    "\n",
    "- Utilizamos \"Inline Dataset\" \n",
    "- Inline Dataset Type: escogemos el tipo de archivo de salida\n",
    "- Linked service: escogemos el Linked Service de destino, y este apunta hacia Azure Blob Storage, a una ruta especifica\n",
    "- Debug mode: al activarlo nos permitirá acceder a las opciones de Mapping y Data preview\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/ZYftvTS6/adf68.png\"></center>\n",
    "\n",
    "- El directorio \"6.Union\" se creará de manera automática dentro del contenedor \"dataflowdataset\"\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/15NCRP5m/adf69.png\"></center>\n",
    "\n",
    "- No particionamos el archivo y se creará un único archivo llamado \"Movie.csv\". Debemos pulsar sobre\n",
    "  el botón de \"Set single partition\" que aparecerá\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/vT7v981f/adf70.png\"></center>\n",
    "\n",
    "\n",
    "##### Ejecución del Pipeline y resultados\n",
    "\n",
    "- Despues de haber finalizado nuestro Data Flow, lo ejecutaremos dentro de una actividad \"Data Flow\" desde un Pipeline\n",
    "- Podemos ver que se ha creado el archivo\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/bJprrzjK/adf71.png\"></center>  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
