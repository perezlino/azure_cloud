{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio - Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "Se tiene los archivos delimitados por coma (CSV) dichos archivos son “Clientes.csv” y “Servicios.csv” contienen información de \n",
    "los clientes y de los servicios que corresponde a cada cliente.\n",
    "\n",
    "Mediante Azure Data Factory con el componente Lookup se requiere saber cuales son los clientes que tienen algún tipo de servicio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "- En términos de Power Query, el objetivo de utilizar \"Lookup o Búsqueda\" es obtener el mismo resultado de 'Combinar Consultas'\n",
    "  de acuerdo a una columna en común, trayendo al destino todas las columnas de ambos origenes, sin duplicar la columna que utilizó\n",
    "  para el match, esa columna la trae solo 1 vez. Aqui no se filtran filas, solo logramos hacer el match de la data entre distintos\n",
    "  archivos. \n",
    "\n",
    "      SOURCE                      LOOKUP                       SINK                         \n",
    " ________________          ___________________          ___________________              \n",
    "|                |        |                   |        |                   |           \n",
    "|  sourceCliente |--------|      Lookup       |--------|    sinkCliente    |\n",
    "|________________| +      |___________________| +      |___________________| \n",
    "\n",
    "      SOURCE                      \n",
    " ________________               \n",
    "|                |            \n",
    "| sourceServicio |\n",
    "|________________| "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sourceCliente\n",
    "\n",
    "- Utilizamos \"Inline Dataset\" (obtenemos el mismo resultado que utilizando \"Dataset\", no se cual es la real diferencia)\n",
    "- Inline Dataset Type: escogemos el tipo de archivo de origen\n",
    "- Linked service: escogemos el Linked Service de origen, y este apunta hacia Azure Blob Storage, a una ruta especifica\n",
    "- Debug mode: al activarlo nos permitirá acceder a las opciones de Projection y Data preview\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/QdChNjWS/adf72.png\"></center>\n",
    "\n",
    "- También activamos la casilla de \"First Row Header\"\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/DwXpPDVn/adf73.png\"></center>\n",
    "\n",
    "- Importamos el schema\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/qMkj3Rwc/adf76.png\"></center>\n",
    "\n",
    "\n",
    "##### sourceServicio\n",
    "\n",
    "- Utilizamos \"Inline Dataset\" (obtenemos el mismo resultado que utilizando \"Dataset\", no se cual es la real diferencia)\n",
    "- Inline Dataset Type: escogemos el tipo de archivo de origen\n",
    "- Linked service: escogemos el Linked Service de origen, y este apunta hacia Azure Blob Storage, a una ruta especifica\n",
    "- Debug mode: al activarlo nos permitirá acceder a las opciones de Projection y Data preview\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/CLNbTsjk/adf74.png\"></center>\n",
    "\n",
    "- También activamos la casilla de \"First Row Header\"\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/bv8XD8ZK/adf75.png\"></center>\n",
    "\n",
    "- Importamos el schema\n",
    "- Cuando una columna de un archivo CSV es del tipo 'date' debemos darle un formato, de lo contrario, esa columna nos \n",
    "  devolverá valores 'NULL'\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/3RfJFWpy/adf77.png\"></center>\n",
    "\n",
    "\n",
    "##### Lookup\n",
    "\n",
    "- Cuando se realicen comparaciones tenemos que 'IMPORTAR LOS SCHEMAS' en las tareas de origen, en \"sourceCliente\" y \n",
    "  \"sourceServicio\"\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/xCnt31YS/adf78.png\"></center>\n",
    "\n",
    "\n",
    "##### sinkCliente\n",
    "\n",
    "- Utilizamos \"Inline Dataset\" \n",
    "- Inline Dataset Type: escogemos el tipo de archivo de salida\n",
    "- Linked service: escogemos el Linked Service de destino, y este apunta hacia Azure Blob Storage, a una ruta especifica\n",
    "- Debug mode: al activarlo nos permitirá acceder a las opciones de Mapping y Data preview\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/15qw4Zh0/adf79.png\"></center>\n",
    "\n",
    "- El directorio \"7.Lookup\" se creará de manera automática dentro del contenedor \"dataflowdataset\"\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/Z5DWL7SY/adf80.png\"></center>\n",
    "\n",
    "- No particionamos el archivo y se creará un único archivo llamado \"ReporteCliente.csv\". Debemos pulsar sobre\n",
    "  el botón de \"Set single partition\" que aparecerá\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/PrVfGCF2/adf81.png\"></center>\n",
    "\n",
    "\n",
    "##### Ejecución del Pipeline y resultados\n",
    "\n",
    "- Despues de haber finalizado nuestro Data Flow, lo ejecutaremos dentro de una actividad \"Data Flow\" desde un Pipeline\n",
    "- Podemos ver que se ha creado el archivo\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/wBgsMygL/adf82.png\"></center>  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
