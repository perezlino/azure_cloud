{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proyecto 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 1 - Creación de un Azure Data Lake Storage y contenedores\n",
    "\n",
    "1. Comenzamos creando un Azure Data Lake Storage\n",
    "\n",
    "2. Luego creamos 3 contenedores con un \"Public level access\" configurado en 'Private':\n",
    "    * raw\n",
    "    * errordata\n",
    "    * validdata    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 2 - Creación de los Linked Services\n",
    "\n",
    "3. Trabajaremos con el siguiente Github:\n",
    "\n",
    "   https://github.com/sarafupm/adf_adb_project1\n",
    "\n",
    "4. Crearemos un Linked Service que haga referencia a un conector HTTP. Nos conectaremos a una cuenta \n",
    "   Github. Solo debemos utilizar la ruta sin el archivo, es decir, si tenemos un repositorio de nombre \n",
    "   \"azure_cloud\" y dentro de él tenemos el archivo \"ProductoCategoria.csv\", debemos acceder al \n",
    "   archivo y desde la opción 'Raw' copiar la dirección de este:\n",
    "\n",
    "   https://github.com/sarafupm/adf_adb_project1/raw/main/IPL%202016.csv\n",
    "\n",
    "   Nosotros solamente necesitamos:\n",
    "\n",
    "   https://github.com/perezlino/azure_cloud/raw/main/\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/FHssm3t6/adf251.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/90MJCLpM/adf252.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/x8zNvHj5/adf253.png\"></center>\n",
    "\n",
    "4. Crearemos un Linked Service que haga referencia a nuestro Data Lake Storage\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/D0wB77hL/adf254.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/DymBDfBK/adf255.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/tJ8P8b4X/adf256.png\"></center>\n",
    "\n",
    "5. De esta manera hemos creado dos Linked Services\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/3w3GsnYD/adf257.png\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 3 - Creación de los Datasets\n",
    "\n",
    "6. Pasos para crear nuestro primer dataset\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/mgBqnWz3/adf258.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/hPgpDbDc/adf259.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/wBKtzFzk/adf260.png\"></center>\n",
    "\n",
    "- Comenzaremos creando el dataset \"ds_IPL_2008_csv\" que hará referencia al archivo \"ipl_2088.csv\"\n",
    "- Indicaremos que el archivo tiene encabezados y no importaremos su schema\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/sfZ7kB5y/adf261.png\"></center>\n",
    "\n",
    "7.\n",
    "- Crearemos un nuevo dataset \"ds_IPL_2008_json\", que al igual que el anterior hara referencia hacia \n",
    "  nuestro ADLS y pero esta vez tendrá un formato de tipo JSON. Hara referencia al archivo \"IPL_2008.json\" \n",
    "  que no existe. No importaremos el schema, dado que el archivo aún no existe, no tiene datos.\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/024gbBb7/adf262.png\"></center>\n",
    "\n",
    "8.\n",
    "- De igual manera, podemos renombrar nuestros datasets de la siguiente manera:\n",
    "  * ds_Source_IPL_2008_csv\n",
    "  * ds_Destination_IPL_2008_json\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/QCH3zGFx/adf263.png\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 4 - Crear un Pipeline y agregar la actividad \"Copy Data\"\n",
    "\n",
    "9. Creamos un pipeline y lo llamaremos \"pl_copy_IPL_DATA_2008\"\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/mrtKRNb8/adf264.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/3JjkC2QS/adf265.png\"></center>\n",
    "\n",
    "10. Agregaremos la actividad \"Copy Data\" a nuestro Pipeline\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/mrjKGL1N/adf266.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/yxgG90Qv/adf267.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/3rz8q1Tr/adf268.png\"></center>\n",
    "\n",
    "11. Vamos a **Validar** y luego ejecutar el pipeline utilizando **Debug**. Vamos a ver que nos \n",
    "    devuelve el **Input**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/gr9G6zDL/adf269.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/05hSC7XD/adf270.png\">\n",
    "        <img src=\"https://i.postimg.cc/6pfGwgWf/adf271.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/vTncKS29/adf272.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/nhFhKzxq/adf273.png\"></center>\n",
    "\n",
    "12. Finalmente el archivo \"IPL_2008.json\" se creará en nuestro ADLS en el contenedor \"raw\"\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/Pqbyg2cg/adf274.png\"></center>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
