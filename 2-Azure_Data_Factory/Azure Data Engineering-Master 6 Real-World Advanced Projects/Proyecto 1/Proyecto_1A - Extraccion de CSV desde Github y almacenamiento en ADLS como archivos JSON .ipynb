{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proyecto 1A - Extracción de múltiples archivos CSV desde Github y almacenamiento en ADLS como archivos JSON"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 1 - Creación de un Azure Data Lake Storage y contenedores\n",
    "\n",
    "1. Comenzamos creando un Azure Data Lake Storage\n",
    "\n",
    "2. Luego creamos 1 contenedor con un **Public level access** configurado en **Private**:\n",
    "    * raw   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 2 - Creación de los Linked Services\n",
    "\n",
    "3. Trabajaremos con el siguiente Github:\n",
    "\n",
    "   https://github.com/sarafupm/adf_adb_project1\n",
    "\n",
    "4. Crearemos un Linked Service que haga referencia a un conector HTTP. Nos conectaremos a una cuenta \n",
    "   Github. Solo debemos utilizar la ruta sin el archivo, es decir, si existe un repositorio de nombre \n",
    "   **adf_adb_project1** y dentro de él tenemos el archivo **IPL 2016.csv**, debemos acceder al \n",
    "   archivo y desde la opción **Raw** copiar la dirección de este:\n",
    "\n",
    "   https://github.com/sarafupm/adf_adb_project1/raw/main/IPL%202016.csv\n",
    "\n",
    "   Nosotros solamente necesitamos:\n",
    "\n",
    "   https://github.com/perezlino/azure_cloud/raw/main/\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/FHssm3t6/adf251.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/90MJCLpM/adf252.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/x8zNvHj5/adf253.png\"></center>\n",
    "\n",
    "4. Crearemos un Linked Service que haga referencia a nuestro Data Lake Storage\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/D0wB77hL/adf254.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/DymBDfBK/adf255.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/tJ8P8b4X/adf256.png\"></center>\n",
    "\n",
    "5. De esta manera hemos creado dos Linked Services\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/3w3GsnYD/adf257.png\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 3 - Creación de los Datasets\n",
    "\n",
    "6. Pasos para crear nuestro primer dataset\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/mgBqnWz3/adf258.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/hPgpDbDc/adf259.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/wBKtzFzk/adf260.png\"></center>\n",
    "\n",
    "- Comenzaremos creando el dataset **ds_IPL_2008_csv** que hará referencia al archivo **ipl 2008.csv**\n",
    "- Indicaremos que el archivo tiene encabezados y no importaremos su schema\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/sfZ7kB5y/adf261.png\"></center>\n",
    "\n",
    "7.\n",
    "- Crearemos un nuevo dataset **ds_IPL_2008_json**, que al igual que el anterior hara referencia hacia \n",
    "  nuestro ADLS y pero esta vez tendrá un formato de tipo JSON. Hara referencia al archivo **IPL_2008.json** \n",
    "  que no existe. No importaremos el schema, dado que el archivo aún no existe, no tiene datos.\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/024gbBb7/adf262.png\"></center>\n",
    "\n",
    "8.\n",
    "- De igual manera, podemos renombrar nuestros datasets de la siguiente manera:\n",
    "  * ds_Source_IPL_2008_csv\n",
    "  * ds_Destination_IPL_2008_json\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/QCH3zGFx/adf263.png\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 4 - Crear un Pipeline y agregar la actividad \"Copy Data\"\n",
    "\n",
    "9. Creamos un pipeline y lo llamaremos **pl_copy_IPL_DATA_2008**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/mrtKRNb8/adf264.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/3JjkC2QS/adf265.png\"></center>\n",
    "\n",
    "10. Agregaremos la actividad **Copy Data** a nuestro Pipeline\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/mrjKGL1N/adf266.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/yxgG90Qv/adf267.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/3rz8q1Tr/adf268.png\"></center>\n",
    "\n",
    "11. Vamos a **Validar** y luego ejecutar el pipeline utilizando **Debug**. Vamos a ver que nos \n",
    "    devuelve el **Input**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/gr9G6zDL/adf269.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/05hSC7XD/adf270.png\">\n",
    "        <img src=\"https://i.postimg.cc/6pfGwgWf/adf271.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/vTncKS29/adf272.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/nhFhKzxq/adf273.png\"></center>\n",
    "\n",
    "12. Finalmente el archivo **IPL_2008.json** se creará en nuestro ADLS en el contenedor **raw**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/Pqbyg2cg/adf274.png\"></center>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 5 - Crear nuevos dataset y agregar una nueva actividad \"Copy Data\" al pipeline\n",
    "\n",
    "13. \n",
    "- Reutilizaremos los datasets **ds_Source_IPL_2008_csv** y **ds_Destination_IPL_2008_json** y los clonaremos\n",
    "    para generar nuevos datasets de origen y destino para el archivo **ipl 2009.csv**. Debemos ajustar estos\n",
    "    datasets para que hagan referencia a este archivo, para ello seguiremos los pasos anteriores. Los archivos\n",
    "    serán:\n",
    "    *   ds_Source_IPL_2009_csv\n",
    "    *   ds_Destination_IPL_2009_json \n",
    "\n",
    "\n",
    "14. Agregaremos una nueva actividad **Copy Data** que hará referencia a estos dos nuevos datasets\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/dV8pYp56/adf275.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/g0cWBGG8/adf276.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/9F3jHdWW/adf277.png\"></center>\n",
    "\n",
    "15. Vamos a **Validar** y luego ejecutar el pipeline utilizando **Debug**.\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/hPCb7rfn/adf278.png\"></center>\n",
    "\n",
    "16. Finalmente el archivo **IPL_2009.json** se creará en nuestro ADLS en el contenedor **raw**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/Y9qCg14c/adf279.png\"></center>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 6 - Crear nuevos dataset, crear un nuevo Pipeline y agregar la actividad \"Lookup\"\n",
    "\n",
    "17. Una forma mucho más eficiente de trabajar con múltiples archivos es utilizando **PARÁMETROS**.\n",
    "    Para ello trabajaremos solo con 2 datasets, uno de origen y de destino y crearemos dos \n",
    "    parámetros, ambos a nivel de dataset, uno en cada dataset.\n",
    "\n",
    "    Añadiremos un parámetro para el archivo **ds_Source_IPL_2008_json** que es el archivo que\n",
    "    utilizaremos como dataset de destino para todos los archivos. Más adelante modificaremos\n",
    "    su nombre\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/KvJP6N2V/adf280.png\"></center> \n",
    "<center><img src=\"https://i.postimg.cc/sx242VtY/adf281.png\"></center>\n",
    "\n",
    "Añadiremos un parámetro para el archivo **ds_Source_IPL_2008_csv** que es el archivo que\n",
    "utilizaremos como dataset de origen para todos los archivos. Más adelante modificaremos\n",
    "su nombre\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/DzKQXQ9y/adf282.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/SK76724L/adf283.png\"></center>\n",
    "\n",
    "18. Modificamos los nombres de nuestros dataset de origen y destino. Quedarán de la siguiente \n",
    "    manera:\n",
    "    *   ds_Source_IPL_csv\n",
    "    *   ds_Destination_IPL_json    \n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/5yHQGjhR/adf284.png\"></center>\n",
    "\n",
    "19. Crearemos un nuevo pipeline, con nuevos ajustes que nos permita mayor eficiencia al momento de\n",
    "    lanzar nuestro proceso. Nuestro pipeline llevará el nombre de **pl_Copy_Full_IPL_DATA** y al cual\n",
    "    le agregamos una actividad **Lookup**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/Hk18ZYB6/adf285.png\"></center>\n",
    "\n",
    "20. Eliminaremos los archivos que se almacenaron en el contenedor **raw** de nuestro ADLS y cargaremos \n",
    "    el archivo **Files.txt**, que es basicamente un archivo que almacena los nombres de todos los archivos\n",
    "    que queremos extraer desde el sitio de Github\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/jjd1fb0j/adf286.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/KzCpC3KK/adf287.png\"></center>\n",
    "\n",
    "21. Crearemos un nuevo dataset que haga referencia hacia el archivo **Files.txt** que recien subimos a nuestro \n",
    "    ADLS, al contenedor **raw**. Este dataset llevará el nombre de **ds_listOfFiles**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/TYVNHwTw/adf288.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/hPc1v0qD/adf289.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/MphcMWj7/adf290.png\"></center>\n",
    "\n",
    "22. Realizamos ajustes a nuestra actividad **Lookup** de nuestro pipeline **pl_Copy_Full_IPL_DATA**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/XvSJNQTx/adf291.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/VkMfhXdt/adf292.png\"></center>\n",
    "\n",
    "23. Vamos a **Validar** y luego ejecutar el pipeline utilizando **Debug**. Revisaremos que nos devuelve\n",
    "    el **Output**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/hPRcdfCP/adf293.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/bw3hWLqs/adf294.png\"></center>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 7 - Agregar la actividad \"ForEach\"\n",
    "\n",
    "24. Agregaremos una actividad **ForEach** a nuestro Pipeline y le realizaremos los ajustes necesarios\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/1t4pyBgW/adf295.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/Qt0KKT7F/adf296.png\"></center>\n",
    "\n",
    "Utilizaremos el contenido dinámico **@activity('Lookup All File Names').output.value** en la configuración\n",
    "de la actividad ForEach, esto quiere decir, que de la actividad previa **Lookup All File Name** extraera\n",
    "solo **value**. Corresponde al array que contiene los nombres de los archivos que utilizaremos.\n",
    "\n",
    "Esto es el **Output** de la actividad previa **Lookup**\n",
    "```\n",
    "{\n",
    "    \"count\":15,\n",
    "    \"value\": [\n",
    "        {\n",
    "            \"Files\":\"ipl 2008.csv\"\n",
    "        }\n",
    "        {\n",
    "            \"Files\":\"ipl 2009.csv\"            \n",
    "        }\n",
    "        ...\n",
    "        ...\n",
    "    ]\n",
    "\n",
    "}\n",
    "```\n",
    "<center><img src=\"https://i.postimg.cc/dtzkCvhj/adf297.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/8CG7yHBF/adf298.png\"></center>\n",
    "\n",
    "25. Ahora, editaremos la tarea que realizará nuestro **ForEach**. Agregamos la actividad **Copy Data**.\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/CKymKWCR/adf299.png\"></center>\n",
    "\n",
    "Para la configuración del origen **Source** se creará el contenido dinámico **@items().Files** que captura \n",
    "los valores de **Files**. Esto es el **Output** de la actividad previa **Lookup**\n",
    "```\n",
    "{\n",
    "    \"count\":15,\n",
    "    \"value\": [\n",
    "        {\n",
    "            \"Files\":\"ipl 2008.csv\"\n",
    "        }\n",
    "        {\n",
    "            \"Files\":\"ipl 2009.csv\"            \n",
    "        }\n",
    "        ...\n",
    "        ...\n",
    "    ]\n",
    "\n",
    "}\n",
    "```\n",
    "\n",
    "Por cada valor de **File** que se tenga, se irá heredando al parámetro del dataset de origen **dataset().SourceFileName**,\n",
    "por tanto, se irá heredando cada valor de **File** como nombres de archivos para el dataset de origen\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/NfSxNWzd/adf300.png\"></center>\n",
    "\n",
    "Para la configuración del destino **Sink** se creará el contenido dinámico **@replace(@items().Files,'.csv','.json')**\n",
    "que tendrá la misma función que el contenido dinámico para el Origen, solo que en este caso se reemplazará su extensión\n",
    "por una JSON.\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/QdkwRQW9/adf301.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/FFM6zWyk/adf302.png\"></center>\n",
    "\n",
    "26. Vamos a **Validar** y luego ejecutar el pipeline utilizando **Debug**.\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/4x7nP3KS/adf303.png\"></center>\n",
    "\n",
    "27. Si revisamos nuestro ADLS, el contenedor **raw** se veria de la siguiente manera\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/nL9RnqXG/adf304.png\"></center>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
