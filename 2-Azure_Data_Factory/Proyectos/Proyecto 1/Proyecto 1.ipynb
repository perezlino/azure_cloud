{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Proyecto 1 - Ingesta y Carga**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Arquitectura del Proyecto**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/g0DZL4kX/adf663.png\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 1 - Data Ingestion**\n",
    "_____\n",
    "\n",
    "1. Pasos a seguir en nuestra Ingesta de datos\n",
    "\n",
    "    *   Crear un contenedor en ADLS donde almacenar la data\n",
    "    *   Crear un Pipeline de ingesta de datos\n",
    "    *   Crear Linked Services de origen y destino\n",
    "    *   Crear datasets de origen y destino"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 1.1 - Creación de un Azure Data Lake Storage y contenedores**\n",
    "\n",
    "1. Comenzamos creando una cuenta de almacenamiento Azure Data Lake Storage\n",
    "\n",
    "2. Luego creamos un contenedor con un **Public level access** configurado en **Private**:\n",
    "    * raw   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 1.2 - Creación de Linked Services de origen y destino**\n",
    "\n",
    "1. Crear el Linked Service de origen **LS_HttpServer_GitHub** que hace referencia al repositorio Github\n",
    "\n",
    "2. Crear el Linked Service de destino **LS_AzureDataLakeStorageG2** que hace referencia a nuestro ADLS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 1.3 - Creación de Datasets de origen y destino para copiar el archivo \"ipl 2008.csv\"**\n",
    "\n",
    "1. Crear el Dataset de origen **ds_Source_IPL_2008_csv** que hará referencia al archivo **ipl 2008.csv** del repositorio Github\n",
    "\n",
    "2. Crear el Dataset de destino **ds_Destination_IPL_2008_json** que hará referencia al archivo **IPL_2008.json** que no existe y que se almacenará en el contenedor **raw**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 1.4 - Creación de Pipeline \"pl_copy_IPL_DATA_2008\"**\n",
    "\n",
    "1. Crear el Pipeline de ingesta de datos **pl_copy_IPL_DATA_2008**, el cual contendrá una actividad **Copy data** y este copiará el archivo **ipl 2008.csv** ubicado en el repositorio Github, en la ruta ADLS **raw/IPL_2008.json**, almacenandolo en formato **JSON**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 1.5 - Modificación de Pipeline \"pl_copy_IPL_DATA_2008\"**\n",
    "\n",
    "1. Reutilizaremos los datasets **ds_Source_IPL_2008_csv** y **ds_Destination_IPL_2008_json** y los clonaremos para generar nuevos datasets de origen y destino para el archivo **ipl 2009.csv** del repositorio Github. Debemos ajustar estos datasets para que hagan referencia a este archivo, para ello seguiremos los pasos anteriores. Los nuevos datasets serán:\n",
    "\n",
    "    *   **ds_Source_IPL_2009_csv**\n",
    "    *   **ds_Destination_IPL_2009_json**\n",
    "\n",
    "   Utilizaremos el mismo pipeline **pl_copy_IPL_DATA_2008** y agregaremos una actividad **Copy data** \n",
    "   \n",
    "   Este archivo se copiará en la ruta ADLS **raw/IPL_2009.json**, almacenandolo en formato **JSON**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 1.6 - Creación de Pipeline \"pl_Copy_FULL_IPL_DATA\"**\n",
    "\n",
    "1.  Una forma mucho más eficiente de trabajar con múltiples archivos es utilizando **PARÁMETROS**. Para ello trabajaremos solo con 2 datasets, uno de origen y de destino y crearemos dos parámetros, ambos a nivel de dataset, uno en cada dataset. Reutilizaremos los datasets **ds_Source_IPL_2008_csv** y **ds_Destination_IPL_2008_json** los cuales haran referencia a un parámetro y no a un archivo. Modificaremos sus nombres:\n",
    "\n",
    "    *   **ds_Source_IPL_csv**\n",
    "    *   **ds_Destination_IPL_json**\n",
    "\n",
    "    Crearemos el pipeline **pl_Copy_FULL_IPL_DATA** el cual utilizará las actividades **Lookup** y **For Each**    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 2 - Data Loading**\n",
    "_____\n",
    "\n",
    "1. Pasos a seguir en nuestra Ingesta de datos\n",
    "\n",
    "    *   Crear Linked Service con referencia hacia Azure SQL Database\n",
    "    *   Crear datasets de origen y destino\n",
    "    *   Crear un Pipeline de carga de datos    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 2.1 - Creación de un Linked Services con referencia hacia Azure SQL Database**\n",
    "\n",
    "1. Crear el Linked Service de destino **LS_AzureSqlDatabase_IPL_db** que hace referencia hacia Azure SQL Database. Reutilizaremos el Linked Service de destino **LS_AzureDataLakeStorageG2** que hace referencia a nuestro ADLS creado en el paso anterior"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 2.2 - Creación de Datasets de origen y destino para cargar el archivo \"ipl 2008.json\"**\n",
    "\n",
    "1. Crear el Dataset de origen **ds_Valid_IPL_Data_Json** que hará referencia al archivo **ipl 2008.json** que se encuentra en nuestro ADLS\n",
    "\n",
    "2. Crear el Dataset de destino **ds_AzureSqlTable_IPL_Data** que hará referencia a la tabla **dbo.tbl_IPLData** en Azure SQL Database"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 2.3 - Creación de Pipeline \"pl_Copy_Valid_IPL_To_SQL\"**\n",
    "\n",
    "1. Creación y configuración de un nuevo Pipeline que se llamará **pl_Copy_Valid_IPL_To_SQL** que nos permitirá realizar la carga del archivo **ipl 2008.json** en la tabla **dbo.tbl_IPLData**. Agregaremos la actividad **Copy Data**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 2.4 - Creación de Pipeline \"pl_Copy_Full_IPL_Data_using_DF\"**\n",
    "\n",
    "1. Cargar todos los archivos JSON del contenedor \"raw\" de nuestro ADLS utilizando Data Flows. Para ello creamos un nuevo Pipeline llamado **pl_Copy_Full_IPL_Data_using_DF** en el cual ejecutaremos nuestro Data Flow\n",
    "\n",
    "    Reutilziaremos el dataset de origen **ds_Valid_IPL_Data_Json** y el dataset de destino **ds_AzureSqlTable_IPL_Data**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
