{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Proyecto 1 - Ingesta, Transformación, Carga**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Arquitectura del Proyecto**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/g0DZL4kX/adf663.png\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 1 - Data Ingestion**\n",
    "_____\n",
    "\n",
    "1. Pasos a seguir en nuestra Ingesta de datos\n",
    "\n",
    "    *   Crear un contenedor en ADLS donde almacenar la data\n",
    "    *   Crear un Pipeline de ingesta de datos\n",
    "    *   Crear Linked Services de origen y destino\n",
    "    *   Crear datasets de origen y destino"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 1.1 - Creación de un Azure Data Lake Storage y contenedores**\n",
    "\n",
    "1. Comenzamos creando una cuenta de almacenamiento Azure Data Lake Storage\n",
    "\n",
    "2. Luego creamos un contenedor con un **Public level access** configurado en **Private**:\n",
    "    * raw   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 1.2 - Creación de Linked Services de origen y destino**\n",
    "\n",
    "1. Crear el Linked Service de origen **LS_HttpServer_GitHub** que hace referencia al repositorio Github\n",
    "\n",
    "2. Crear el Linked Service de destino **LS_AzureDataLakeStorageG2** que hace referencia a nuestro ADLS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 1.3 - Creación de Datasets de origen y destino para copiar el archivo \"ipl 2008.csv\"**\n",
    "\n",
    "1. Crear el Dataset de origen **ds_Source_IPL_2008_csv** que hará referencia al archivo **ipl 2008.csv** del repositorio Github\n",
    "\n",
    "2. Crear el Dataset de destino **ds_Destination_IPL_2008_json** que hará referencia al archivo **IPL_2008.json** que no existe y que se almacenará en el contenedor **raw**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 1.4 - Creación de Pipeline \"pl_copy_IPL_DATA_2008\"**\n",
    "\n",
    "1. Crear el Pipeline de ingesta de datos **pl_copy_IPL_DATA_2008**, el cual contendrá una actividad **Copy data** y este copiará el archivo **ipl 2008.csv** ubicado en el repositorio Github, en la ruta ADLS **raw/IPL_2008.json**, almacenandolo en formato **JSON**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 1.5 - Modificación de Pipeline \"pl_copy_IPL_DATA_2008\"**\n",
    "\n",
    "1. Reutilizaremos los datasets **ds_Source_IPL_2008_csv** y **ds_Destination_IPL_2008_json** y los clonaremos para generar nuevos datasets de origen y destino para el archivo **ipl 2009.csv** del repositorio Github. Debemos ajustar estos datasets para que hagan referencia a este archivo, para ello seguiremos los pasos anteriores. Los nuevos datasets serán:\n",
    "\n",
    "    *   **ds_Source_IPL_2009_csv**\n",
    "    *   **ds_Destination_IPL_2009_json**\n",
    "\n",
    "   Utilizaremos el mismo pipeline **pl_copy_IPL_DATA_2008** y agregaremos una actividad **Copy data**. \n",
    "   \n",
    "   Este archivo se copiará en la ruta ADLS **raw/IPL_2009.json**, almacenandolo en formato **JSON**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 1.6 - Creación de Pipeline \"pl_Copy_FULL_IPL_DATA\"**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
