{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### **Paso 4.2 - Mejoras en el Synapse notebook**"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Crear un notebook y utilizando un Spark Pool conectarse a ADLS"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%pyspark\n","from notebookutils import mssparkutils\n","from pyspark.sql.functions import *"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### ====== Esto se agregó al Pipeline ======"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"b450298a-2cc5-405d-9328-52188db9aa4e","showTitle":false,"title":""}},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- ctime: timestamp (nullable = false)\n","\n","+-----------------------+\n","|ctime                  |\n","+-----------------------+\n","|2023-05-30 22:22:40.921|\n","+-----------------------+\n","\n"]}],"source":["import datetime\n","\n","# Esto es Python no es Spark\n","dateFormat = \"%Y-%m-%d\"\n","\n","ts = spark.sql(\"SELECT CURRENT_TIMESTAMP() AS ctime\")\n","\n","ts.printSchema() # ctime: timestamp (nullable = false)\n","\n","ts.show(truncate=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"1ded212b-91df-4c59-baf3-9d51323f174b","showTitle":false,"title":""}},"outputs":[{"name":"stdout","output_type":"stream","text":["Out[18]: datetime.datetime(2023, 5, 30, 21, 54, 47, 142000)"]}],"source":["# Primera forma para obtener el timestamp\n","\n","ts.collect() # [Row(ctime=datetime.datetime(2023, 5, 30, 21, 38, 4, 538000))]\n","\n","ts.collect()[0][\"ctime\"] # datetime.datetime(2023, 5, 30, 21, 38, 4, 538000)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"46e19ec1-c064-4fa5-a18a-bbb0c5086ad3","showTitle":false,"title":""}},"outputs":[],"source":["ts = spark.sql(\"SELECT CURRENT_TIMESTAMP() AS ctime\").collect()[0][\"ctime\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"7704cb8a-25fc-4d78-9f85-4fbc4cb6d399","showTitle":false,"title":""}},"outputs":[{"name":"stdout","output_type":"stream","text":["Out[36]: datetime.datetime(2023, 5, 30, 22, 16, 19, 957000)"]}],"source":["# Segunda forma para obtener el timestamp\n","\n","from pyspark.sql.functions import current_date, current_timestamp\n","\n","ts = ts.select(current_timestamp().alias('ctime'))\n","\n","ts.collect()[0][\"ctime\"] # datetime.datetime(2023, 5, 30, 22, 0, 51, 624000)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"6d4dbdd3-211b-44e3-a1bb-0371f2549179","showTitle":false,"title":""}},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-05-30\n"]}],"source":["# Obtenemos la fecha --> Timestamp a Date --> Pero este 'Date' es un STRING\n","# Esta función es de Python, no es Spark\n","\n","todaydate = ts.strftime(dateFormat)\n","print(todaydate)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### ==================================="]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%pyspark\n","sourceaccountName = \"ottadls011\"\n","sourcecontainer = \"raw\"\n","sourceLinkedService = \"LS_ADLS_transform\"\n","sourceFile_location = \"ingest/\"+todaydate      <------------------ Agregamos '+todaydate'"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-secure-credentials-with-tokenlibrary?pivots=programming-language-python\n","<center><img src=\"https://i.postimg.cc/tT45hpCj/adf530.png\"></center>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%pyspark\n","spark.conf.set('spark.storage.synapse.linkedServiceName', 'LS_ADLS_transform')\n","spark.conf.set('fs.azure.account.oauth.provider.type', 'com.microsoft.azure.synapse.tokenlibrary.LinkedServiceBasedTokenProvider')\n","\n","path = f'abfss://{sourcecontainer}@{sourceaccountName}.dfs.core.windows.net/'\n","\n","print(path) # abfss://raw@ottadls011.dfs.core.windows.net/"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%pyspark\n","file = f'abfss://{sourcecontainer}@{sourceaccountName}.dfs.core.windows.net/{sourceFile_location}'\n","\n","print(file) # abfss://raw@ottadls011.dfs.core.windows.net/ingest/2023-05-30"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%pyspark\n","source_df = spark.read.format(\"csv\") \\\n","                      .option(\"inferSchema\",True) \\\n","                      .option(\"header\",True) \\\n","                      .option(\"sep\",\",\") \\\n","                      .load(file)"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":4},"notebookName":"Untitled Notebook 2023-05-30 17:36:41","widgets":{}},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
