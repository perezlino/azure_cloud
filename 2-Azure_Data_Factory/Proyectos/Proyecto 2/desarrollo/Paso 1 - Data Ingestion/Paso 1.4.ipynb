{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 1.4 - Creación de un Pipeline\n",
    "\n",
    "1. Crear el Pipeline de ingesta de datos **PL_Onprem_ADLS_Ingest**, el cual contendrá una actividad **Copy data**\n",
    "\n",
    "2. Crearemos un Pipeline más eficiente realizando una **Carga Incremental (Incremental load)** basandonos en la última fecha    \n",
    "   modificada. Para ello, necesitaremos crear un nuevo dataset de origen llamado **DS_Onprem_File_param** y un dataset de destino llamado **DS_ADLS_ingest_param** que utilizarán parámetros."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crear el Pipeline de ingesta de datos **PL_Onprem_ADLS_Ingest**, el cual contendrá una actividad **Copy data**\n",
    "\n",
    "En la pestaña **Source** de la actividad **Copy data** tenemos la opción de **Wildcard file path** al cual le ingresamos el comodín ***.csv**, de esta manera tomará TODOS LOS ARCHIVOS CON EXTENSIÓN .CSV en la ruta especificada.\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/X7x9tbLM/adf495.png\"></center>\n",
    "\n",
    "En la pestaña **Sink** no hacemos mayores cambios, más que ingresar el dataset de destino\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/zBmhTXzh/adf496.png\"></center>\n",
    "\n",
    "Luego **Validamos** y ejecutamos nuestro pipeline utilizando **Debug**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/x85brvKG/adf497.png\"></center>\n",
    "\n",
    "Podemos ver que se han copiado todos los archivos con extensión **.csv** desde la ruta **\\host** en nuestro entorno local hacia ADLS a la ruta **raw/ingest**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/ncdQ01Vp/adf498.png\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crearemos un Pipeline más eficiente realizando una **Carga Incremental (Incremental load)** de archivos basandonos en la última fecha modificada\n",
    "\n",
    "- **Carga Incremental (Incremental load)**\n",
    "\n",
    "    * Actividad de CARGAR SOLO NUEVOS REGISTROS desde un origen a un Destino.\n",
    "    * La carga incremental es mucho más rápida que la carga completa (Full load) y también consume relativamente menos recursos.\n",
    "    * Requiere menos tiempo.\n",
    "    \n",
    "\n",
    "##### **Carga Incremental: Primera Forma**\n",
    "\n",
    "- **Carga incremental de archivos usando la última fecha modificada**\n",
    "\n",
    "    * Los archivos se añadirán a la fuente diariamente\n",
    "    * Usando la última fecha modificada del archivo podemos hacer carga incremental de datos\n",
    "    * Azure Data Factory seleccionará sólo los archivos modificados de acuerdo a la fecha que nosotros le indiquemos\n",
    "\n",
    "Podemos ver que el último archivo modificado que fue copiado a ADLS fue **2023-02-16 094712.950119.csv**. Ahora agregamos el archivo **2023-02-23 200147.874793.csv** al directorio **\\Host** de nuestro sistema local. Este vendria a ser el último archivo modificado en nuestro sistema local.\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/qRNH8478/adf499.png\"></center>\n",
    "\n",
    "Para efectos del ejemplo, suponer que la fecha actual (la fecha de hoy) es 23/02/2023, por tanto, filtraremos por la última fecha modificada, indicaremos que queremos que filtre SOLO POR EL DIA DE HOY, entonces para **Start time** utilizamos contenido dinámico **@startOfDay(utcnow())**:\n",
    "```\n",
    "Start time (UTC): @startOfDay(utcnow()) ---> utcnow(): \"2023-02-23T13:34:17.0000000Z\"\n",
    "                                        ---> startOfDay(utcnow()) --> startOfDay('2023-02-23T13:34:17.0000000Z') \n",
    "                                                                  --> \"2023-02-23T00:00:00.0000000Z\"\n",
    "```\n",
    "Y para **End time** indicaremos:\n",
    "```\n",
    "End time (UTC): @utcnow() ---> utcnow(): \"2023-02-23T13:34:17.0000000Z\"\n",
    "```\n",
    "Utilizando la opción **Filter by last modified** de esta manera solo filtrará archivos que fueron modificados en la fuente de origen en el dia de hoy **2023-02-23**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/vTXKjKDQ/adf500.png\"></center>\n",
    "\n",
    "De esta manera solo se cargará ESE ARCHIVO ÚNICO y NO TODOS LOS ARCHIVOS NUEVAMENTE\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/W14dQqYj/adf501.png\"></center>\n",
    "\n",
    "\n",
    "##### **Carga Incremental: Segunda Forma**\n",
    "\n",
    "- **Carga incremental de archivos usando la fecha en el nombre del archivo**\n",
    "\n",
    "    * También podemos realizar la carga incremental utilizando el formato del nombre del archivo \n",
    "    * Extrayendo la fecha del nombre del archivo se puede realizar una carga incremental\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/s2X0Qmn8/adf502.png\"></center>\n",
    "\n",
    "Vamos a empezar creando un nuevo Pipeline que realice esta carga incremental, se llamará **PL_Ingest_Incremental_Load**.\n",
    "Primero agregaremos una actividad **Get Metadata**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/0Q2Tdm8M/adf503.png\"></center>\n",
    "\n",
    "Luego agregaremos una actividad **For each** que capturará los **childItems** que devuelve la actividad de **Get Metadata**. Los **childItems** corresponden a los nombres de los archivos que se encuentran en la fuente de origen, es por eso, que la actividad **Get Metadata** utiliza el dataset de origen.\n",
    "```\n",
    "@activity('Get Metadata1').output.childItems\n",
    "```\n",
    "<center><img src=\"https://i.postimg.cc/W1stLW8Q/adf504.png\"></center>\n",
    "\n",
    "Vamos a trabajar en la actividad **For each**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/1zkrH10k/adf505.png\"></center>\n",
    "\n",
    "Tenemos la siguiente **Expression**:\n",
    "```\n",
    "equals(): Devuelve \"true\" si dos valores son iguales\n",
    "item(): esta referenciando a \"Items\" de Settings\n",
    "item().name: dado que \"Items\" captura los \"childItems\" que devuelve la actividad \"Get Metadata\", 'name' hace referencia al nombre del archivo\n",
    "\n",
    "@equals(\n",
    "    substring(item().name,0,10),\n",
    "    formatDateTime(utcnow(),yyyy-MM-dd)\n",
    "    )\n",
    "\n",
    "substring(item().name,0,10) --> substring('2023-02-23 200147.874793.csv',0,10) --> \"2023-02-23\"\n",
    "formatDateTime(utcnow(),yyyy-MM-dd) --> formatDateTime('2023-02-23T13:34:17.0000000Z', yyyy-MM-dd) --> \"2023-02-23\"\n",
    "```\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/W306YYXh/adf506.png\"></center>\n",
    "\n",
    "Vamos a trabajar en la opción de **True**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/q72TJnLW/adf507.png\"></center>\n",
    "\n",
    "Previamente, debemos crear un nuevo dataset de origen llamado **DS_Onprem_File_param** que utilizará parámetros. También hará referencia a **File system** y al tipo de archivo **Delimited Text**. Utilizará el Linked Service de origen y la misma ruta hacia **\\Host**. Debemos marcar la casilla de **First Row as Header**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/7hzH2cMY/adf508.png\"></center>\n",
    "\n",
    "Creamos un parámetro a nivel de dataset de origen\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/qMM00zWv/adf509.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/Y9zKBH3F/adf510.png\"></center>\n",
    "\n",
    "Debemos crear un nuevo dataset de destino llamado **DS_ADLS_ingest_param** que utilizará parámetros. También hará referencia a **Azure Data Lake Storage Gen2** y al tipo de archivo **Delimited Text**. Utilizará el Linked Service de destino y la misma ruta hacia **raw/ingest**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/CMCDXQVV/adf511.png\"></center>\n",
    "\n",
    "Creamos un parámetro a nivel de dataset de destino\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/d3Q0zn8q/adf512.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/xTCC5xf5/adf513.png\"></center>\n",
    "\n",
    "El parámetro del dataset de origen **FileName** tomará el valor **@{item().name}**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/c4yKxM0B/adf514.png\"></center>\n",
    "\n",
    "El parámetro del dataset de destino **FileName** tomará el valor **@{item().name}**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/RFJF9qYY/adf515.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/DfsZMJLJ/adf516.png\"></center>\n",
    "\n",
    "Antes de **Validar** y ejecutar con **Debug**, eliminamos el archivo **2023-02-16 094712.950119.csv** de nuestro ADLS, cargado anteriormente. Al ejecutarse, irá SECUENCIALMENTE (como indicamos en la actividad **For each**) archivo por archivo e irá verificando la **Expresión**. Vemos que el ÚLTIMO ARCHIVO ejecutó el **Copy data** dado que es el único archivo que cumplío la Expresión de la actividad **If Condition**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/s2vf9v32/adf517.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/mZQLwxW7/adf518.png\"></center>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
