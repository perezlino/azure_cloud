{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Proyecto 2 - Ingesta, Transformación, Carga y Reportería**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Arquitectura del Proyecto**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/t4j2B2gs/adf455.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/rpygksxc/adf456.png\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 1 - Data Ingestion**\n",
    "_____\n",
    "\n",
    "1. Crear un Self-hosted Integrated runtime en ADF\n",
    "\n",
    "2. Descargar y configurar el Self-hosted IR en nuestro entorno local\n",
    "\n",
    "3. Pasos a seguir en nuestra Ingesta de datos\n",
    "    *   Crear un contenedor en ADLS donde almacenar la data\n",
    "    *   Crear un Pipeline de ingesta de datos\n",
    "    *   Crear Linked Services de origen y destino\n",
    "    *   Crear datasets de origen y destino\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/MZgJXZMw/adf458.png\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 1.1 - Azure Key Vault**\n",
    "\n",
    "1. Crear el recurso de Azure Key Vault\n",
    "\n",
    "2. Crear directivas de acceso para Azure Data Factory (Access Policies) en Azure Key Vault\n",
    "\n",
    "3. Crear secretos de usuario y contraseña para utilizarlo en la conexión del Linked Service con nuestro entorno local\n",
    "\n",
    "4. Crear secreto para la Access Key (Account Key) de nuestra cuenta de almacenamiento ADLS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 1.2 - Creación de Linked Services de origen y destino**\n",
    "\n",
    "1. Crear el Linked Service **LS_Keyvault** que hace referencia a Azure Key Vault\n",
    "\n",
    "2. Crear el Linked Service de origen **LS_Onprem_File** que hace referencia al almacenamiento de nuestro local\n",
    "\n",
    "3. Crear el Linked Service de destino **LS_adls_ingest** que hace referencia a nuestro ADLS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 1.3 - Creación de Datasets de origen y destino**\n",
    "\n",
    "1. Crear el Dataset de origen **DS_Onprem_File** que hace referencia al almacenamiento de nuestro local\n",
    "\n",
    "2. Crear el Dataset de destino **DS_adls_ingest** que hace referencia a nuestro ADLS, a la ruta **raw/ingest**. \n",
    "    Siendo **raw** el contenedor y **ingest** el directorio"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 1.4 - Creación de un Pipeline**\n",
    "\n",
    "1. Crear el Pipeline de ingesta de datos **PL_Onprem_ADLS_Ingest**, el cual contendrá una actividad **Copy data**\n",
    "\n",
    "2. Crearemos un Pipeline más eficiente realizando una **Carga Incremental (Incremental load)** basandonos en la última fecha    \n",
    "   modificada. Para ello, necesitaremos crear un nuevo dataset de origen llamado **DS_Onprem_File_param** y un dataset de destino llamado **DS_ADLS_ingest_param** que utilizarán parámetros.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 2 - Data Transformation**\n",
    "_____\n",
    "\n",
    "1. Pasos en la transformación de datos\n",
    "\n",
    "    *   Crear una aplicación Azure Synapse Analytics\n",
    "    *   Crear un Synapse Notebook\n",
    "    *   Leer los datos que están presentes en Azure Datalake.\n",
    "    *   Aplicar la lógica de transformación\n",
    "    *   Escribir los datos en Azure Datalake\n",
    "```\n",
    "```\n",
    "2. Lectura de datos del ADLS desde Synapse Notebook\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/bwn6dD7K/adf519.png\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 2.1 - Creación de Linked Service que haga referencia hacia ADLS**\n",
    "\n",
    "1. Acceder a ADLS usando **Managed Identity**\n",
    "\n",
    "2. Configurar la **Managed Identity** de Azure Synapse Analytics y asignarle un **Rol** para que tenga acceso a **ADLS** \n",
    "\n",
    "3. Crear el Linked Service **LS_ADLS_transform** que hace referencia a ADLS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 2.2 - Creación de un Synapse notebook y leer los datos de ADLS utilizando Spark Pool**\n",
    "\n",
    "1. Crear un notebook y conectarse a ADLS usando **PySpark**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 2.3 - Comenzar la lógica de transformación de los datos**\n",
    "\n",
    "1. Comenzar la lógica de transformación de los datos\n",
    "\n",
    "    *   Identificar y eliminar filas duplicadas\n",
    "    *   Reemplazar valores Null\n",
    "    *   Creación de nuevas columnas\n",
    "    *   Cambio de tipo de datos\n",
    "    *   Renombrar columnas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 2.4 - Escribir datos transformados en Datalake**\n",
    "\n",
    "1. Escribir los datos ya transformados en nuestro ADLS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 2.5 - Llamar al Synapse notebook desde Azure Data Factory**\n",
    "\n",
    "1. Ambos **Managed Identity** de **ADF** y **User** deben tener acceso **Synapse Administrator** en **Synapse Analytics**\n",
    "\n",
    "2. Crear un Linked Service que haga referencia a nuestro workspace de Azure Synapse     \n",
    "\n",
    "3. Uso de la actividad **Notebook** en Azure Data Factory (ADF)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 3 - Data Loading**\n",
    "_____\n",
    "\n",
    "1. Acceder a Azure SQL Database desde SSMS\n",
    "\n",
    "2. Cargar datos hacia Azure SQL Database\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/h4Myztq2/adf564.png\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 3.1 - Acceder a Azure SQL Database desde SSMS**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 3.2 - Creación de Linked Service que haga referencia hacia Azure SQL Database**\n",
    "\n",
    "1. Enmascarar nuestro Password de acceso al servidor de Azure SQL Database utilizando Azure Key Vault\n",
    "\n",
    "2. Crear el Linked Service de destino **LS_SQL_Load** que hace referencia a Azure SQL Database. Reutilizaremos el Linked Service de origen **LS_adls_ingest** que hace referencia a nuestro ADLS y también el Linked Service **LS_Keyvault** que hace referencia a Azure Key Vault"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 3.3 - Creación de Datasets de origen y destino**\n",
    "\n",
    "1. Crear el Dataset de origen **DS_ADLS_Refined** que hace referencia a nuestro ADLS, a la ruta **refined/data**. \n",
    "    Siendo **refined** el contenedor y **data** el directorio\n",
    "\n",
    "2. Crear el Dataset de destino **DS_SQL_Load** que hace referencia a Azure SQL Database"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 3.3 - Creación de un Pipeline**\n",
    "\n",
    "1. Crear el Pipeline de ingesta de datos **PL_ADLS_to_SQL**, el cual contendrá una actividad **Copy data**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 4 - Mejoras**\n",
    "_____\n",
    "\n",
    "1. Mejoras para la copia desde on-premise a ADLS \n",
    "\n",
    "    *   Copiar el archivo de hoy a una carpeta específica en ADLS mientras se ingiere desde on-premise.\n",
    "    *   El nombre de la carpeta debe ser la fecha de hoy (la misma fecha que el archivo).\n",
    "    *   Todos los archivos de la fecha de hoy se almacenarán en una carpeta que tenga la fecha de hoy.\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/BvqSBfpZ/adf571.png\"></center>   \n",
    "\n",
    "2. Mejoras en el Synapse notebook\n",
    "\n",
    "    *   La transformación se hará sólo a ese archivo en particular ya que la carpeta tendrá sólo el archivo de hoy\n",
    "    *   Los recursos de computación se pueden guardar en Spark\n",
    "    *   El tiempo de ejecución se reducirá ya que sólo se computarán las filas de hoy\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/k4YYhWdY/adf572.png\"></center>     "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 4.1 - Mejoras para la copia desde on-premise a ADLS**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 4.2 - Mejoras en el Synapse notebook**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 5 - Orquestación**\n",
    "_____\n",
    "\n",
    "1. Orquestar todos los Pipelines creados\n",
    "\n",
    "    *   Crear un nuevo Pipeline llamado **PL_Orchestrate_all** y agregar la actividad **Execute Pipeline**\n",
    "    *   Utilizar un Trigger **Tumbling Window**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/s2TSnpLG/adf580.png\"></center>  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 5.1 - Orquestar todos los Pipelines creados**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 5.2 - Enviar una notificación automática de alerta por correo electrónico cuando falla el pipeline en ADF**\n",
    "\n",
    "- Como el pipeline está ahora automatizado, necesitamos saber si el pipeline ha fallado\n",
    "- El correo electrónico de alerta debe enviarse a un grupo o a un usuario individual\n",
    "- El correo electrónico de alerta debe tener:\n",
    "\n",
    "    *   El nombre del pipeline\n",
    "    *   Data Factory name\n",
    "    *   Run ID\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 6 - Reporteria (Power BI)**\n",
    "_____\n",
    "\n",
    "1. Utilizar los datos que cargamos en Azure SQL Database en un reporte que crearemos en Power BI\n",
    "\n",
    "    *   Abrir Power BI Desktop\n",
    "    *   Obtener datos desde Azure SQL Database desde Power BI\n",
    "    *   Autenticación\n",
    "    *   Cargar datos en el modelo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 6.1 - Crear un reporte en Power BI**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 7 - Configuring Continuous Integration / Continuous deployment**\n",
    "_____\n",
    "\n",
    "**Continuous Integration**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/SQvC35WQ/adf620.png\"></center>  \n",
    "\n",
    "**Continuous Deployment**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/vDwBMWYh/adf621.png\"></center>  \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
