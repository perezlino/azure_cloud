{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Proyecto 3 - Ingesta, Transformación, Carga y Reportería**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Arquitectura del Proyecto**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/t4j2B2gs/adf455.png\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 1 - Data Ingestion**\n",
    "_____\n",
    "\n",
    "1. Pasos a seguir en nuestra Ingesta de datos desde Azure Blob Storage hacia Azure Data Lake Storage Gen2\n",
    "\n",
    "    *   Crear un contenedor en ABS donde almancenar la data\n",
    "    *   Crear un contenedor en ADLS donde almacenar la data\n",
    "    *   Crear Linked Services de origen y destino\n",
    "    *   Crear datasets de origen y destino    \n",
    "    *   Crear un Pipeline de ingesta de datos    \n",
    "\n",
    "\n",
    "<center><img src=\"https://images2.imgbox.com/a7/8b/zClZtz13_o.png\"></center> <!-- adf664 -->\n",
    "\n",
    "2. Pasos a seguir en nuestra Ingesta de datos desde HTTP hacia Azure Data Lake Storage Gen2\n",
    "\n",
    "    *   Reutilizaremos contenedor en ADLS donde almacenar la data\n",
    "    *   Crear un Linked Service de origen y reutilizar el Linked Service de destino\n",
    "    *   Crear datasets parametrizados tanto de origen como destino \n",
    "    *   Crear un Pipeline de ingesta de datos parametrizado    \n",
    "\n",
    "\n",
    "<center><img src=\"https://images2.imgbox.com/3e/19/JBr4g1NM_o.png\"></center> <!-- adf721 -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 1.1 - Creación de Linked Services de origen y destino para la ingesta de datos desde Azure Blob Storage hacia Azure Data Lake Storage Gen2**\n",
    "\n",
    "1. Crear el Linked Service de origen **ls_ablob_covidreportingsa** que hace referencia a Azure Blob Storage\n",
    "\n",
    "2. Crear el Linked Service de destino **ls_adls_covidreportingdl** que hace referencia a nuestro ADLS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 1.2 - Creación de Datasets de origen y destino para la ingesta de datos desde Azure Blob Storage hacia Azure Data Lake Storage Gen2**\n",
    "\n",
    "1. Crear el Dataset de origen **ds_population_raw_gz** que hace referencia al archivo **population_by_age.tsv.gz** alojado en el contenedor **population** en Azure Blob Storage\n",
    "\n",
    "2. Crear el Dataset de destino **ds_population_raw_tsv** que hace referencia al archivo **population_by_age.tsv** (que aún no existe, pero se   \n",
    "   creará de manera automática al ejecutar el pipeline) a nuestro ADLS, a la ruta **raw/population**. Siendo **raw** el contenedor y **population** el directorio"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 1.3 - Creación de un Pipeline para la ingesta de datos desde Azure Blob Storage hacia Azure Data Lake Storage Gen2**\n",
    "\n",
    "1. Crear el Pipeline de ingesta de datos **pl_ingest_population_data**, el cual contendrá una actividad **Copy data**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 1.4 - Escenarios extras**\n",
    "\n",
    "1. Ejecutar la actividad **Copy data** cuando el archivo esté disponible\n",
    "\n",
    "2. Ejecutar la actividad **Copy data** sólo si el contenido del archivo es el esperado\n",
    "\n",
    "3. Eliminar el archivo de origen al copiarlo correctamente"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 1.5 - Creación de un Linked Service de origen para la ingesta de datos desde HTTP hacia Azure Data Lake Storage Gen2**\n",
    "\n",
    "1. Crear el Linked Service de origen **ls_http_opendata_ecdc_europa_eu** que hace referencia a HTTP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 1.6 - Creación de Datasets de origen y destino parametrizados para la ingesta de datos desde HTTP hacia Azure Data Lake Storage Gen2**\n",
    "\n",
    "1. Crear el Dataset de origen **ds_ecdc_raw_csv_http** que hace referencia al archivo parametrizado **@dataset().relativeURL**. Dicho valor del   \n",
    "   parámetro será indicado al momento de ejecutar el pipeline\n",
    "\n",
    "2. Crear el Dataset de destino **ds_ecdc_raw_csv_dl** que hace referencia al archivo parametrizado **@dataset().fileName** (que aún no existe,    \n",
    "   pero se creará de manera automática al ejecutar el pipeline) de nuestro ADLS, aque se almacenará en la ruta **raw/ecdc**. Siendo **raw** el contenedor y **ecdc** el directorio"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 1.7 - Creación de un Pipeline parametrizado para la ingesta de datos desde HTTP hacia Azure Data Lake Storage Gen2**\n",
    "\n",
    "1. Crear el Pipeline de ingesta de datos parametrizado **pl_ingest_ecdc_data**, el cual contendrá una actividad **Copy data**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 1.8 - Creación de un Trigger \"Schedule\" que se anexará al pipeline \"pl_ingest_ecdc_data\"**\n",
    "\n",
    "1. Crear el Trigger **Schedule** y que se llamará **tr_ingest_hospital_admissions_data** y se anexará al pipeline **pl_ingest_ecdc_data**. Al   \n",
    "   ejecutarse el trigger solo ingestará datos de **hospital_admissions.csv**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 1.9 - Parametrizar el Linked Service de origen \"ls_http_opendata_ecdc_europa_eu\" para la ingesta de datos desde HTTP hacia Azure Data Lake Storage Gen2**\n",
    "\n",
    "1. Parametrizar el Linked Service de origen **ls_http_opendata_ecdc_europa_eu**\n",
    "\n",
    "2. Crear un nuevo parámetro en el dataset de origen **ds_ecdc_raw_csv_http**\n",
    "\n",
    "3. Crear un nuevo parámetro en el pipeline **pl_ingest_ecdc_data**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 1.10 - Modificación final al proceso de ingesta desde HTTP**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 2 - Data Transformation (Data Flow)**\n",
    "_____\n",
    "\n",
    "1. Pasos en la transformación de datos del archivo **cases_deaths.csv**\n",
    "\n",
    "    *   Crear un dataflow para el proceso de transformación\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/MKvdgTnT/adf780.png\"></center>\n",
    "\n",
    "2. Pasos en la transformación de datos del archivo **hospital-admissions.csv**\n",
    "\n",
    "    *   Crear un dataflow para el proceso de transformación\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/5tqD1hMN/adf848.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/C1hXKcFC/adf849.png\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 2.1 - Creamos el Data flow \"df_transform_cases_deaths\" para transformar los datos del archivo \"cases_deaths.csv\"**\n",
    "\n",
    "1. Configurar el stream **Source** y crear un nuevo dataset de origen que haga referencia al archivo **cases_deaths.csv**, llamado  \n",
    "   **ds_raw_cases_and_deaths**\n",
    "\n",
    "2. Agregar un stream **Filter** para filtrar el campo **continent** y nos devuelva solo registros para el continente **Europe**\n",
    "\n",
    "3. Agregar un stream **Select** para seleccionar las columnas que necesitamos\n",
    "\n",
    "4. Agregar un stream **Pivot**\n",
    "\n",
    "5. Agregar un stream **Lookup** y un stream **Source**. Además, crear un nuevo dataset de origen que haga referencia al archivo **country_lookup.csv**, llamado **ds_country_lookup**\n",
    "\n",
    "6. Nuevamente, agregar un stream **Select** para seleccionar y ordenar las columnas que necesitamos   \n",
    "\n",
    "7. Y para finalizar, utilizaremos un stream **Sink** y creamos un un nuevo dataset de destino que haga referencia a la ruta **processed/ecdc/cases_deaths** de nuestro ADLS. No se indicó un nombre de archivo\n",
    "\n",
    "8. Creamos un pipeline llamado **pl_process_cases_and_deaths_data*** para ejecutar el Data flow recién creado"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 2.2 - Creamos el Data flow \"df_transform_cases_deaths\" para transformar los datos del archivo \"hospital-admissions.csv\"**\n",
    "\n",
    "1. Configurar el stream **Source** y crear un nuevo dataset de origen que haga referencia al archivo **hospital-admissions.csv**, llamado  \n",
    "   **ds_raw_hospital_admission**\n",
    "\n",
    "2. Agregar un stream **Select** para renombrar y eliminar campos\n",
    "\n",
    "3. Agregar un stream **Lookup** y un stream **Source**. El stream **Source** hace referencia al archivo **country_lookup.csv**, más especificamente al dataset **ds_country_lookup**\n",
    "\n",
    "4. Agregar un stream **Select** para eliminar un campo\n",
    "\n",
    "5. Agregar un stream **Split Conditional**\n",
    "\n",
    "6. Agregar y configurar un nuevo stream **Source** y crear un nuevo dataset de origen que haga referencia al archivo **dim_date.csv**, llamado **ds_dim_date_lookup** \n",
    "\n",
    "7. Agregar un stream **Derived Column** para crear un nuevo campo\n",
    "\n",
    "8. Agregar un stream **Aggregate**\n",
    "\n",
    "9. Agregar un stream **Join**\n",
    "\n",
    "10. Agregar un stream **Pivot** llamado **PivotWeekly**\n",
    "\n",
    "11. Agregar un stream **Pivot** llamado **PivotDaily**\n",
    "\n",
    "12. Agregar un stream **Sort** llamado **SortWeekly**\n",
    "\n",
    "13. Agregar un stream **Sort** llamado **SortDaily**\n",
    "\n",
    "14. Agregar un stream **Select** llamado **SelectWeekly** para renombrar columnas\n",
    "\n",
    "15. Agregar un stream **Select** llamado **SelectDaily** para renombrar columnas\n",
    "\n",
    "7. Y para finalizar, utilizaremos un stream **Sink** llamado **WeeklySink**  y creamos un un nuevo dataset de destino que haga referencia a la ruta **processed/ecdc/hospital_admission_weekly** de nuestro ADLS. No se indicó un nombre de archivo. Y un stream **Sink** llamado **DailySink**  y creamos un un nuevo dataset de destino que haga referencia a la ruta **processed/ecdc/hospital_admission_daily** de nuestro ADLS. Tampoco se indicó un nombre de archivo.\n",
    "\n",
    "8. Creamos el pipeline **pl_process_hospital_admissions_data** para ejecutar el Data flow recién creado"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 2.3 - Modificaciones**\n",
    "\n",
    "1. Modificar el dataset **ds_ecdc_file_list**\n",
    "\n",
    "2. Modificar el dataset **ds_raw_cases_and_deaths**\n",
    "\n",
    "2. Modificar el dataset **ds_raw_hospital_admission**\n",
    "\n",
    "2. Modificar el dataset **ds_country_lookup**\n",
    "\n",
    "3. Modificar el dataset **ds_dim_date_lookup**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 3 - Data Transformation (HDInsight)**\n",
    "_____"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 3.1 - Utilizar un Cluster HDInsight para transformar datos utilizando HIVE**\n",
    "\n",
    "1. Crear un recurso **User Assigned Managed Identity** \n",
    "\n",
    "2. Generar acceso basado en un rol al recurso de Managed Identity que hemos creado en el ADLS **covidreportingdl**\n",
    "\n",
    "3. Crear un recurso **Azure HDInsight Cluster**\n",
    "\n",
    "4. Crear el pipeline **pl_process_testing_data** con una actividad **Hive**. También, crear el Linked Service **ls_hdi_covid_cluster** que haga referencia al Cluster HDInsight"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 4 - Data Transformation (Databricks)**\n",
    "_____"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 4.1 - Mount storage**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 4.2 - Transformar los datos del archivo population.tsv**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 4.3 - Crear el pipeline que ejecute un notebook de Databricks**\n",
    "\n",
    "1. Crear el Linked Service **ls_db_covid_cluster** que haga referencia a Databricks. Utilizaremos una autenticación del tipo **Access Token**.\n",
    "\n",
    "2. Crear el pipeline **pl_process_population_data** con una actividad **Notebook**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
