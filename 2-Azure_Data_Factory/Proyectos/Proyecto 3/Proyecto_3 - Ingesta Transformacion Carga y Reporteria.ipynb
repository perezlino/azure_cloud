{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Proyecto**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Arquitectura del Proyecto**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/t4j2B2gs/adf455.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/rpygksxc/adf456.png\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Paso 1 - Data Ingestion**\n",
    "_____\n",
    "\n",
    "1. Crear un Self-hosted Integrated runtime en ADF\n",
    "\n",
    "2. Descargar y configurar el Self-hosted IR en nuestro entorno local\n",
    "\n",
    "3. Pasos a seguir en nuestra Ingesta de datos\n",
    "    *   Crear un contenedor en ADLS donde almacenar la data\n",
    "    *   Crear un Pipeline de ingesta de datos\n",
    "    *   Crear Linked Services de origen y destino\n",
    "    *   Crear datasets de origen y destino\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/MZgJXZMw/adf458.png\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 1.1 - Azure Key Vault**\n",
    "\n",
    "1. Crear el recurso de Azure Key Vault\n",
    "\n",
    "2. Crear directivas de acceso para Azure Data Factory (Access Policies) en Azure Key Vault\n",
    "\n",
    "3. Crear secretos de usuario y contraseña para utilizarlo en la conexión del Linked Service con nuestro entorno local\n",
    "\n",
    "4. Crear secreto para la Access Key (Account Key) de nuestra cuenta de almacenamiento ADLS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 1.2 - Creación de Linked Services de origen y destino**\n",
    "\n",
    "1. Crear el Linked Service **LS_Keyvault** que hace referencia a Azure Key Vault\n",
    "\n",
    "2. Crear el Linked Service de origen **LS_Onprem_File** que hace referencia al almacenamiento de nuestro local\n",
    "\n",
    "3. Crear el Linked Service de destino **LS_adls_ingest** que hace referencia a nuestro ADLS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 1.3 - Creación de Datasets de origen y destino**\n",
    "\n",
    "1. Crear el Dataset de origen **DS_Onprem_File** que hace referencia al almacenamiento de nuestro local\n",
    "\n",
    "2. Crear el Dataset de destino **DS_adls_ingest** que hace referencia a nuestro ADLS, a la ruta **raw/ingest**. \n",
    "    Siendo **raw** el contenedor e **ingest** el directorio"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 1.4 - Creación de un Pipeline**\n",
    "\n",
    "1. Crear el Pipeline de ingesta de datos **PL_Onprem_ADLS_Ingest**, el cual contendrá una actividad **Copy data**\n",
    "\n",
    "2. Crearemos un Pipeline más eficiente realizando una **Carga Incremental (Incremental load)** basandonos en la última fecha    \n",
    "   modificada. Para ello, necesitaremos crear un nuevo dataset de origen llamado **DS_Onprem_File_param** y un dataset de destino llamado **DS_ADLS_ingest_param** que utilizarán parámetros.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Paso 2 - Data Transformation**\n",
    "_____\n",
    "\n",
    "1. Pasos en la transformación de datos\n",
    "\n",
    "    *   Crear una aplicación Azure Synapse Analytics\n",
    "    *   Crear un Synapse Notebook\n",
    "    *   Leer los datos que están presentes en Azure Datalake.\n",
    "    *   Aplicar la lógica de transformación\n",
    "    *   Escribir los datos en Azure Datalake\n",
    "```\n",
    "```\n",
    "2. Lectura de datos del ADLS desde Synapse Notebook\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/bwn6dD7K/adf519.png\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 2.1 - Creación de Linked Service que haga referencia hacia ADLS**\n",
    "\n",
    "1. Configurar la **Managed Identity** de Azure Synapse Analytics y asignarle un **Rol** para que tenga acceso a **ADLS** \n",
    "\n",
    "2. Crear el Linked Service **LS_ADLS_transform** que hace referencia a ADLS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 2.2 - Creación de un Synapse notebook y leer los datos de ADLS utilizando Spark Pool**\n",
    "\n",
    "1. Crear un notebook y utilizando un Spark Pool conectarse a ADLS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 2.3 - Comenzar la lógica de transformación de los datos**\n",
    "\n",
    "1. Comenzar la lógica de transformación de los datos\n",
    "\n",
    "    *   Identificar y eliminar filas duplicadas\n",
    "    *   Reemplazar valores Null\n",
    "    *   Creación de nuevas columnas\n",
    "    *   Cambio de tipo de datos\n",
    "    *   Renombrar columnas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 2.4 - Escribir datos transformados en Datalake**\n",
    "\n",
    "1. Escribir los datos ya transformados en nuestro ADLS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 2.5 - Llamar al Synapse notebook desde Azure Data Factory**\n",
    "\n",
    "1. Llamar al Synapse notebook desde Azure Data Factory\n",
    "\n",
    "    *   Uso de la actividad **Notebook** en Azure Data Factory (ADF).\n",
    "    *   Ambos **Managed Identity** de **ADF** y **User** deben tener acceso **Synapse Administrator** en **Synapse Analytics**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
