{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio - Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "Utilice la transformación de Join para combinar datos de dos fuentes de Origen. El flujo de salida incluirá todas las columnas \n",
    "de ambas fuentes que coincidan en función de una condición de combinación. Se requiere unir multiples orígenes de datos mediante \n",
    "el componente Join de Azure Data Factory para generar un archivo de salida y depositarlo en el servicio de Azure Blob Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "      SOURCE                                  JOIN                            SINK\n",
    " ________________                      ___________________          _________________________        \n",
    "|                |                    |                   |        |                         |      \n",
    "| SourceProducto |--------------------|     InnerJoin     |--------|  sinkProductoInnerJoin  |\n",
    "|________________| +                  |___________________| +      |_________________________|\n",
    "                                                |\n",
    "      SOURCE                                    |\n",
    " ____________________________                   |\n",
    "|                            |                  |\n",
    "| SourceSubCategoriaProducto |------------------'\n",
    "|____________________________|        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sourceProduct\n",
    "\n",
    "- Utilizamos \"Inline Dataset\" (obtenemos el mismo resultado que utilizando \"Dataset\", no se cual es la real diferencia)\n",
    "- Inline Dataset Type: escogemos el tipo de archivo de origen\n",
    "- Linked service: escogemos el Linked Service de origen, y este apunta hacia Azure Blob Storage, a una ruta especifica\n",
    "- Debug mode: al activarlo nos permitirá acceder a las opciones de Projection y Data preview\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/k56mF9pp/adf21.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/52GctT2q/adf22.png\"></center>\n",
    "\n",
    "- Importamos el schema\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/85B43KXV/adf25.png\"></center>\n",
    "\n",
    "\n",
    "##### sourceSubCategoriaProducto\n",
    "\n",
    "- Inline Dataset Type: escogemos el tipo de archivo de origen\n",
    "- Linked service: escogemos el Linked Service de destino, y este apunta hacia Azure Blob Storage, a una ruta especifica\n",
    "- Debug mode: al activarlo nos permitirá acceder a las opciones de Projection y Data preview\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/5NLpzrgS/adf23.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/5NVKs16R/adf24.png\"></center>\n",
    "\n",
    "- Importamos el schema\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/j2Txq5h7/adf26.png\"></center>\n",
    "\n",
    "\n",
    "##### InnerJoin\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/sgH5BFqX/adf27.png\"></center>\n",
    "\n",
    "\n",
    "##### sinkProductoInnerJoin\n",
    "\n",
    "- Inline Dataset Type: escogemos el tipo de archivo de salida\n",
    "- Linked service: escogemos el Linked Service de destino, y este apunta hacia ABS, a una ruta de datos especifica\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/k54GbBK3/adf28.png\"></center>\n",
    "\n",
    "- El directorio \"3.Join\" se creará de manera automática dentro del contenedor \"dataflowdataset\"\n",
    "<center><img src=\"https://i.postimg.cc/C1Sv7pMg/adf29.png\"></center>\n",
    "\n",
    "- No particionamos el archivo y se creará un único archivo llamado \"ProductInnerJoin.csv\". Debemos pulsar sobre\n",
    "  el botón de \"Set single partition\" que aparecerá\n",
    "<center><img src=\"https://i.postimg.cc/LsNPccBR/adf30.png\"></center>\n",
    "\n",
    "\n",
    "##### Ejecución del Pipeline y resultados\n",
    "\n",
    "- Despues de haber finalizado nuestro Data Flow, lo ejecutaremos dentro de una actividad \"Data Flow\" desde un Pipeline\n",
    "- Podemos ver que se han creado los dos archivos  \n",
    "<center><img src=\"https://i.postimg.cc/vmVBzCq3/adf31.png\"></center>  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
