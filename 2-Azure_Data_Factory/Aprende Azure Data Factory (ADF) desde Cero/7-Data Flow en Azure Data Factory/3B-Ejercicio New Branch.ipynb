{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio - New Branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "Se requiere que te conectes a la tabla Producty mediante el componente New Branch de Azure Data Factory se necesita que se \n",
    "generen distintos archivos de salida en archivo delimitado por comas (csv) y archivo de json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "- Si modificamos la raiz original (SourceProduct), se modificará también la rama\n",
    "\n",
    "      SOURCE                      SINK                    \n",
    " ________________          ___________________        \n",
    "|                |        |                   |     \n",
    "|  SourceProduct |--------|  sinkProductCSV   |\n",
    "|________________| +      |___________________|\n",
    "\n",
    "    NEW BRANCH                    SINK\n",
    " ________________          ___________________            \n",
    "|                |        |                   |           \n",
    "|  SourceProduct |--------|  sinkProductJSON  |\n",
    "|________________| +      |___________________|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sourceProduct\n",
    "\n",
    "- Utilizamos \"Inline Dataset\" (obtenemos el mismo resultado que utilizando \"Dataset\", no se cual es la real diferencia)\n",
    "- Inline Dataset Type: escogemos el recurso de donde proviene el Dataset de origen\n",
    "- Linked service: escogemos el Linked Service de origen, y este apunta hacia Azure SQL Database, a una base de datos especifica\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/ZnY7K58s/adf9.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/rmTcj5Td/adf10.png\"></center>\n",
    "\n",
    "\n",
    "##### sinkProductCSV\n",
    "\n",
    "- Inline Dataset Type: escogemos el tipo de archivo de salida\n",
    "- Linked service: escogemos el Linked Service de destino, y este apunta hacia ABS, a una ruta de datos especifica\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/28hscVHh/adf11.png\"></center>\n",
    "\n",
    "- El directorio \"2.New Branch\" se creará de manera automática dentro del contenedor \"dataflowdataset\"\n",
    "<center><img src=\"https://i.postimg.cc/44bvWG6n/adf12.png\"></center>\n",
    "\n",
    "- De esta manera, no particionamos el archivo y se creará un único archivo llamado \"Product.csv\". Debemos pulsar sobre\n",
    "  el botón de \"Set single partition\" que aparecerá\n",
    "<center><img src=\"https://i.postimg.cc/3R3NRsdb/adf13.png\"></center>\n",
    "\n",
    "\n",
    "##### sinkProductJSON\n",
    "\n",
    "- Inline Dataset Type: escogemos el tipo de archivo de salida\n",
    "- Linked service: escogemos el Linked Service de destino, y este apunta hacia ABS, a una ruta de datos especifica\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/T14nndbP/adf14.png\"></center>\n",
    "\n",
    "- Escribimos la misma ruta que la tarea \"sinkProductCSV\" donde se almacenarán los archivos\n",
    "- No particionamos el archivo y se creará un único archivo llamado \"Product.json\". Debemos pulsar sobre\n",
    "  el botón de \"Set single partition\" que aparecerá\n",
    "<center><img src=\"https://i.postimg.cc/yd5NPkL2/adf15.png\"></center>\n",
    "\n",
    "\n",
    "##### Ejecución del Pipeline y resultados\n",
    "\n",
    "- Debug mode: al activarlo nos permitirá acceder a las opciones de Mapping y Data preview\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/J7Sw59Ly/adf16.png\"></center>\n",
    "\n",
    "- Despues de haber finalizado nuestro Data Flow, lo ejecutaremos dentro de una actividad \"Data Flow\" desde un Pipeline\n",
    "- Podemos ver que se han creado los dos archivos  \n",
    "<center><img src=\"https://i.postimg.cc/gJwSw2rg/adf17.png\"></center>  \n",
    "\n",
    "- ¿Qué pasa si ahora queremos eliminar ciertas columnas de la tabla de la fuente de origen?\n",
    "- Desde la tarea \"sourceProduct\"\n",
    "- Importamos el schema\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/YStBmYwb/adf19.png\"></center>\n",
    "\n",
    "- Este paso lo realizamos tanto en \"sinkProductCSV\" como en \"sinkProductJSON\"\n",
    "- Desmarcamos la casilla de \"Auto-Mapping\"\n",
    "- Eliminamos las últimas 4 columnas\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/6pctcYcD/adf20.png\"></center>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
