{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio - Alter Row (Upsert if)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "Del ejercicio anterior, se quiere que se realicen algunas Inserciones o Actualizaciones de registros mediante la \n",
    "transformación \"AlterRow\".\n",
    "\n",
    "Se tiene un archivo de Excel y se requiere que se inserten registros a la tabla si la data no existe y si existe se requiere \n",
    "que se actualice la información en base al archivo de Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "          SOURCE                     ALTER ROW                     SINK                \n",
    " ______________________          __________________          __________________             \n",
    "|                      |        |                  |        |                  |      \n",
    "|  SourceEmpleadoNuevo |--------|     AlterRow     |--------|   sinkEmpleado   |\n",
    "|______________________| +      |__________________| +      |__________________|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sourceEmpleadoNuevo\n",
    "\n",
    "- Utilizamos \"Inline Dataset\" (obtenemos el mismo resultado que utilizando \"Dataset\", no se cual es la real diferencia)\n",
    "- Inline Dataset Type: escogemos el tipo de origen\n",
    "- Linked service: escogemos el Linked Service de origen, y este apunta hacia Azure Blob Storage, a una ruta especifica\n",
    "- Debug mode: al activarlo nos permitirá acceder a las opciones de Projection y Data preview. \n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/jSbYbHHQ/adf218.png\"></center>\n",
    "\n",
    "- Si se selecciona un archivo Excel como origen, debemos indicar el nombre de Hoja de ese libro Excel\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/RV0nTt6d/adf219.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/vBQmZR5h/adf220.png\"></center>\n",
    "\n",
    "- Importamos el schema\n",
    "- No fue necesario dar un formato a los campos con tipo \"date\"\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/dtvKMRmR/adf221.png\"></center>\n",
    "\n",
    "\n",
    "##### AlterRow\n",
    "\n",
    "- Condition: Upsert if -> true()   --> Para que aplique esta condición a TODOS los registros. De nuestro archivo de origen Excel, \n",
    "                                       se insertarán registros a la tabla si estos no existen en ella y si existen se actualizará \n",
    "                                       la información en base al archivo de Excel.          \n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/nr95BrJ1/adf222.png\"></center>\n",
    "\n",
    "\n",
    "##### sinkEmpleado\n",
    "\n",
    "- Utilizamos \"Inline Dataset\" (obtenemos el mismo resultado que utilizando \"Dataset\", no se cual es la real diferencia)\n",
    "- Inline Dataset Type: escogemos el tipo de salida\n",
    "- Linked service: escogemos el Linked Service de destino, y este apunta hacia Azure SQL Database, a una base de datos especifica\n",
    "- Debug mode: al activarlo nos permitirá acceder a las opciones de Mapping y Data preview. Además permitirá cargar los schemas\n",
    "              y tablas de la base de datos.\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/1X4gHxCB/adf223.png\"></center>\n",
    "\n",
    "- Se cargará en la tabla que creamos 'Empleado'\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/Zq7vyKjn/adf224.png\"></center>\n",
    "\n",
    "- En base a la columna 'ID' realizará el upsert de registros\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/4xYDTWM0/adf225.png\"></center>\n",
    "\n",
    "\n",
    "##### Ejecución del Pipeline y resultados\n",
    "\n",
    "- Despues de haber finalizado nuestro Data Flow, lo ejecutaremos dentro de una actividad \"Data Flow\" desde un Pipeline\n",
    "- Podemos ver que se ha cargado en la tabla\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/ZKGcCjn2/adf226.png\"></center> \n",
    "<center><img src=\"https://i.postimg.cc/zBYvhbKJ/adf227.png\"></center> "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
