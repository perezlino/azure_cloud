{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio - Derived Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "Se requiere obtener los datos de Azure SQL, los datos son del “Producto” y la “Categoria del Producto” y se requiere obtener \n",
    "el “Monto Total” mediante una nueva columna y también transformar a Mayusculas las columnas.\n",
    "\n",
    "Finalmente se tiene que exportar a Azure Blob Storage en un Archivo Delimitado por Comas (CSV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "          SOURCE                      DERIVED COLUMN                       SINK                         \n",
    " _________________________          ___________________          _______________________              \n",
    "|                         |        |                   |        |                       |           \n",
    "| sourceCategoriaProducto |--------|   DerivedColumn   |--------| sinkCategoriaProducto |\n",
    "|_________________________| +      |___________________| +      |_______________________| "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sourceCategoriaProducto\n",
    "\n",
    "- Utilizamos \"Inline Dataset\" (obtenemos el mismo resultado que utilizando \"Dataset\", no se cual es la real diferencia)\n",
    "- Inline Dataset Type: escogemos el tipo de origen\n",
    "- Linked service: escogemos el Linked Service de origen, y este apunta hacia Azure SQL Database, a una base de datos especifica\n",
    "- Debug mode: al activarlo nos permitirá acceder a las opciones de Projection y Data preview. También nos permitirá visualizar\n",
    "              las tablas en Source options\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/2jndkZpj/adf83.png\"></center>\n",
    "\n",
    "- Pegamos la Query\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/25vyQn7V/adf84.png\"></center>\n",
    "\n",
    "- Importamos el schema\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/wv2622yT/adf85.png\"></center>\n",
    "\n",
    "\n",
    "##### Derived Column\n",
    "\n",
    "- Cuando se escojan las columnas tenemos que 'IMPORTAR LOS SCHEMAS' en la tarea de origen, en \"sourceCategoriaProducto\"\n",
    "- Se modificaron 2 columnas: 'Categoria' y 'Producto'\n",
    "- Se creó una nueva columna 'MontoTotal' en base a dos columnas: 'UnitPrice' y 'UnitPriceDiscount'\n",
    "- Podemos modificar el nombre de una columna, reescribiendo su nombre, por ejemplo, aqui se escribió 'CATEGORIA' en mayusculas\n",
    "- La función \"upper\" convierte a mayusculas los valores de una columna\n",
    "- La función \"toDecimal\" nos devuelve solo 2 números decimales\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/pdGvnTgp/adf86.png\"></center>\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/3JKtzByQ/adf87.png\"></center>\n",
    "\n",
    "\n",
    "##### sinkCategoriaProducto\n",
    "\n",
    "- Utilizamos \"Inline Dataset\" (obtenemos el mismo resultado que utilizando \"Dataset\", no se cual es la real diferencia)\n",
    "- Inline Dataset Type: escogemos el tipo de archivo de salida\n",
    "- Linked service: escogemos el Linked Service de destino, y este apunta hacia Azure Blob Storage, a una ruta especifica\n",
    "- Debug mode: al activarlo nos permitirá acceder a las opciones de Mapping y Data preview.\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/pTHfzWzM/adf88.png\"></center>\n",
    "\n",
    "- El directorio \"8.DerivedColumn\" se creará de manera automática dentro del contenedor \"dataflowdataset\"\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/VNBryjr2/adf89.png\"></center>\n",
    "\n",
    "- No particionamos el archivo y se creará un único archivo llamado \"ProductoCategoria.csv\". Debemos pulsar sobre\n",
    "  el botón de \"Set single partition\" que aparecerá\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/HscrWG8w/adf90.png\"></center>\n",
    "\n",
    "\n",
    "##### Ejecución del Pipeline y resultados\n",
    "\n",
    "- Despues de haber finalizado nuestro Data Flow, lo ejecutaremos dentro de una actividad \"Data Flow\" desde un Pipeline\n",
    "- Podemos ver que se ha creado el archivo\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/cHCsDYQR/adf91.png\"></center>  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
