{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Escribir datos obtenidos desde ADLS en Azure Synapse SQL Dedicated Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "# Cuenta de almacenamiento: datalake2000 \n",
    "# Contenedor: data \n",
    "# Directorio: raw \n",
    "# scope: data-lake-key <-- Nombre de nuestro Databricks Scoped Secret \n",
    "# key: datalake2000 <-- Nombre de nuestro secreto en Azure Key Vault\n",
    "\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.datalake2000.dfs.core.windows.net\",\n",
    "    dbutils.secrets.get(scope=\"data-lake-key\",key=\"datalake2000\"))\n",
    "\n",
    "val df = spark.read.format(\"csv\")\n",
    ".options(Map(\"inferSchema\"->\"true\",\"header\"->\"true\"))\n",
    ".load(\"abfss://data@datalake2000.dfs.core.windows.net/raw/Log.csv\")\n",
    "\n",
    "val dfcorrect = df.select(col(\"Id\"),\n",
    "                          col(\"Correlationid\"),\n",
    "                          col(\"Operationname\"),\n",
    "                          col(\"Status\"),\n",
    "                          col(\"Eventcategory\"),\n",
    "                          col(\"Level\"),\n",
    "                          col(\"Time\"),\n",
    "                          col(\"Subscription\"),\n",
    "                          col(\"Eventinitiatedby\"),\n",
    "                          col(\"Resourcetype\"),\n",
    "                          col(\"Resourcegroup\"))\n",
    "\n",
    "# Cuenta de almacenamiento: datalake2000 \n",
    "# Contenedor: tmpdir \n",
    "# Directorio: log \n",
    "\n",
    "val tablename = \"logdata\"\n",
    "val tmpdir = \"abfss://tmpdir@datalake2000.dfs.core.windows.net/log\"\n",
    "\n",
    "# Esta es la conexión a nuestro Azure Synapse dedicated SQL pool\n",
    "# Azure Synapse Workspace: appworkspace9000\n",
    "# Nombre de nuestra base de datos: newpool\n",
    "# Usuario: sqladminuser\n",
    "# Contraseña: Azure@123\n",
    "\n",
    "val connection = \"jdbc:sqlserver://appworkspace9000.sql.azuresynapse.net:1433;database=newpool;user=sqladminuser;password=Azure@123;encrypt=true;trustServerCertificate=false;\"\n",
    "\n",
    "# Podemos utilizar la función de escritura para escribir a un external data store\n",
    "dfcorrect.write\n",
    "  .mode(\"append\") // Here we are saying to append to the table\n",
    "  .format(\"com.databricks.spark.sqldw\")\n",
    "  .option(\"url\", connection)\n",
    "  .option(\"tempDir\", tmpdir) # Para transferir a Azure Synapse, necesitamos temporary storage para el staging data\n",
    "  .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n",
    "  .option(\"dbTable\", tablename)\n",
    "  .save()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
