{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Proyecto 1 - Ingesta, Transformación, Carga y Reportería**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Arquitectura del Proyecto**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/PJBMKtgL/db127.png\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 1 - Integración con ADLS**\n",
    "_____\n",
    "\n",
    "1. Pasos a seguir para realizar la integración con ADLS\n",
    "\n",
    "2. Codificar el montaje (mount) con ADLS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 2 - Data Ingestion**\n",
    "_____\n",
    "\n",
    "1. Ingesta manual de datos desde API hacia el contenedor **raw** en ADLS\n",
    "\n",
    "2. Pasos a seguir en nuestra Ingesta de archivos desde el contenedor **raw** hacia el contenedor **ingested** (ambos en ADLS)\n",
    "\n",
    "<center><img src=\"https://images2.imgbox.com/a1/9f/t0LTa2k6_o.png\"></center> <!--db61-->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte A"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 2.1 - Ingesta del archivo \"circuits.csv\"**\n",
    "\n",
    "1. Leer el archivo\n",
    "\n",
    "2. Seleccionar solo las columnas que necesitamos\n",
    "\n",
    "3. Cambiar el nombre de ciertas columnas\n",
    "\n",
    "4. Añadir la fecha de ingestión al dataframe\n",
    "\n",
    "5. Escribir datos en el contenedor **processed** del ADLS como **parquet**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/B6dF4Thd/db58.png\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 2.2 - Ingesta del archivo \"races.csv\"**\n",
    "\n",
    "1. Leer el archivo\n",
    "\n",
    "2. Añadir las columnas \"ingestion_date\" y \"race_timestamp\"\n",
    "\n",
    "3. Seleccionar sólo las columnas necesarias y renombrarlas como corresponda\n",
    "\n",
    "4. Escribir datos en el contenedor **processed** del ADLS como **parquet**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/PqNskYvb/db59.png\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 2.3 - Ingesta del archivo \"constructors.json\"**\n",
    "\n",
    "1. Leer el archivo\n",
    "\n",
    "2. Eliminar las columnas no deseadas\n",
    "\n",
    "3. Cambiar el nombre de las columnas y añadir \"ingestion date\"\n",
    "\n",
    "4. Escribir datos en el contenedor **processed** del ADLS como **parquet**\n",
    "\n",
    "<center><img src=\"https://images2.imgbox.com/44/b5/oUpevSA8_o.png\"></center> <!--db62-->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 2.4 - Ingesta del archivo \"drivers.json\"**\n",
    "\n",
    "1. Leer el archivo\n",
    "\n",
    "2. Renombrar columnas y añadir nuevas columnas\n",
    "\n",
    "3. Eliminar las columnas no deseadas\n",
    "\n",
    "4. Escribir datos en el contenedor **processed** del ADLS como **parquet**\n",
    "\n",
    "<center><img src=\"https://images2.imgbox.com/e4/f7/eGqClVvp_o.png\"></center> <!--db63-->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 2.5 - Ingesta del archivo \"results.json\"**\n",
    "\n",
    "1. Leer el archivo\n",
    "\n",
    "2. Renombrar columnas y añadir nuevas columnas\n",
    "\n",
    "3. Eliminar las columnas no deseadas\n",
    "\n",
    "4. Escribir datos en el contenedor **processed** del ADLS como **parquet**\n",
    "\n",
    "<center><img src=\"https://images2.imgbox.com/bd/ee/T5xzXTnY_o.png\"></center> <!--db64-->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 2.6 - Ingesta del archivo \"pit_stops.json\"**\n",
    "\n",
    "1. Leer el archivo\n",
    "\n",
    "2. Renombrar columnas y añadir nuevas columnas\n",
    "\n",
    "3. Escribir datos en el contenedor **processed** del ADLS como **parquet**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/dV8W94f3/db65.png\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 2.7 - Ingesta de los archivos \"lap_times_split.csv\"**\n",
    "\n",
    "1. Leer el directorio **lap_times** el cual contiene multiples archivos CSV\n",
    "\n",
    "2. Renombrar columnas y añadir nuevas columnas\n",
    "\n",
    "3. Escribir datos en el contenedor **processed** del ADLS como **parquet**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/ZntjGQ01/db66.png\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 2.8 - Ingesta de los archivos \"qualifying_split.json\"**\n",
    "\n",
    "1. Leer el directorio **qualifying** el cual contiene multiples archivos Multi Line JSON\n",
    "\n",
    "2. Renombrar columnas y añadir nuevas columnas\n",
    "\n",
    "3. Escribir datos en el contenedor **processed** del ADLS como **parquet**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/pV94qxzr/db67.png\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte B"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 2.9 - Databricks Workflows**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 2.9.1 - En primer lugar, veremos cómo incluir un notebook en otro notebook con parámetros de configuración y funciones comunes**\n",
    "\n",
    "En cualquier lenguaje de programación, es importante escribir código reutilizable para que un proyecto sea más manejable y Spark no es una excepción. Spark nos permite reutilizar un notebook en otro notebook usando el comando mágico **%run**. Y esto ya lo hemos visto anteriormente, pero queremos aplicarlo a nuestro proyecto para que nuestra comprensión sea más concreta."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 2.9.2 - Definir parámetros en un notebook**\n",
    "\n",
    "Vamos a ver cómo pasar parámetros a los notebooks de Databricks. En el punto 2.9.1 establecimos que el comando **%run** nos daba la capacidad de reutilizar código. Por ejemplo, si estás procesando datos de dos fuentes diferentes con las mismas características, en lugar de escribir dos notebooks diferentes, puedes escribir un notebook y enviar el nombre de la fuente de datos como parámetro. En nuestro proyecto, vamos a suponer que podemos obtener los datos de la \"API eargast\", como hemos comentado antes, o de la página web oficial de la Fórmula Uno directamente. Queremos procesar ambos conjuntos de datos utilizando los mismos notebooks, pero queremos almacenar el nombre de la fuente de datos junto a los datos para poder identificar el origen de los datos para cualquier propósito explicativo. En este caso, podemos parametrizar todos nuestros notebooks con el nombre de la fuente de datos como parámetro y enviar el valor en tiempo de ejecución. Los \"widgets\" de Databricks nos ayudan a hacerlo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 2.9.3 - Invocar un notebook desde otro y encadenarlos y pasarles también los parámetros que hemos definido antes**\n",
    "\n",
    "Ahora que tenemos ocho notebooks diferentes y que hasta ahora los hemos estado ejecutando a mano, puede que te estés preguntando cómo se ejecutan en escenarios de producción. Databricks nos brinda jobs que puedes programar para que se ejecuten a una hora específica, a un intervalo regular, lo que está muy bien, pero carece de muchas funcionalidades que normalmente obtendrías con cualquier tipo de herramientas de programación. Por ejemplo, no puedes crear dependencias entre jobs. Para solucionar eso, Databricks nos brinda una utilidad, llamada **Notebook Utility**, que te permite crear notebook workflow. Dicho esto, si estás en un Azure, no es probable que estés usando Databricks para crear workflows y programar jobs (scheduling jobs) en producción, debido a su funcionalidad inferior. Azure nos brinda una herramienta de workflow mucho mejor llamada **Azure Data Factory**, y eso es lo que utilizaremos. También me gustaría llevarte a través del notebook workflow y Databricks jobs para que puedas crear jobs rápidos para testear workloads sin tener que usar Azure Data Factory. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 2.9.4 - Crear databricks jobs para ejecutar notebooks a intervalos regulares y bajo demanda**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al final de este proceso, se espera lograr una buena comprensión de las capacidades que ofrece Databricks para crear notebook workflows y ejecutarlos desde databricks jobs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte C"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 2.10 - Crear tablas con Databricks SQL**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 2.10.1 - Crearemos EXTERNAL TABLES para todos los archivos que se encuentren en el directorio \"raw\" del ADLS** \n",
    "\n",
    "La base de datos **f1_raw** se creará por defecto en la ruta **/user/hive/warehouse/f1_raw.db**\n",
    "\n",
    "1. Una **external table** que haga referencia al archivo **circuits.csv** ubicada en la ruta **/mnt/formula1dl/raw/circuits.csv**\n",
    "\n",
    "2. Una **external table** que haga referencia al archivo **races.csv** ubicada en la ruta **/mnt/formula1dl/raw/races.csv**\n",
    "\n",
    "3. Una **external table** que haga referencia al archivo **constructors.json** ubicada en la ruta **/mnt/formula1dl/raw/constructors.json**\n",
    "\n",
    "4. Una **external table** que haga referencia al archivo **drivers.json** ubicada en la ruta **/mnt/formula1dl/raw/drivers.json**\n",
    "\n",
    "5. Una **external table** que haga referencia al archivo **results.json** ubicada en la ruta **/mnt/formula1dl/raw/results.json**\n",
    "\n",
    "6. Una **external table** que haga referencia al archivo **pit_stops.json** ubicada en la ruta **/mnt/formula1dl/raw/pit_stops.json**\n",
    "\n",
    "7. Una **external table** que haga referencia al directorio **lap_times** ubicada en la ruta **/mnt/formula1dl/raw/lap_times**\n",
    "\n",
    "8. Una **external table** que haga referencia al directorio **qualifying** ubicada en la ruta **/mnt/formula1dl/raw/qualifying**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 2.10.2 - Crearemos MANAGED TABLES para todos los archivos ingestados y modificados**\n",
    "\n",
    "La base de datos **f1_processed** se creará en la ruta **/mnt/formula1dl/processed**\n",
    "\n",
    "1. Modificaremos la ruta de escritura de todos los notebooks, para escribir los datos ingestados en su tabla correspondiente\n",
    "\n",
    "2. Debemos eliminar el directorio y archivo parquet creado previamente para todos los archivos, dado que, al guardarlo como tabla creará el directorio y archivo y además creará la tabla\n",
    "\n",
    "2. Utilizaremos **lenguaje python** para crear las tablas, por lo que, por defecto serán **MANAGED TABLES**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 3 - Data Transformation**\n",
    "_____\n",
    "\n",
    "1. Modificar nombres de columnas que no realizamos en la etapa de **Data Ingestion**\n",
    "\n",
    "2. Utilizar las funciones **Filter** y **Join** para transformar los datos\n",
    "```\n",
    "    Utilizaremos la función \"Join\" sobre estos 5 dataframes\n",
    "```    \n",
    "<center><img src=\"https://i.postimg.cc/tC7fWR4B/db85.png\"></center>\n",
    "\n",
    "```\n",
    "    Utilizaremos los siguientes campos\n",
    "```\n",
    "<center><img src=\"https://i.postimg.cc/7YRzX2FJ/db86.png\"></center>\n",
    "\n",
    "3. Utilizar las funciones **GroupBy** y **Agg** para transformar los datos\n",
    "\n",
    "4. Utilizar las **Window functions** para transformar los datos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte A"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 3.1 - Utilizar las funciones \"Filter\" y \"Join\" para transformar los datos y asi obtener datos organizados que nos muestren los resultados de las carreras**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 3.2 - Utilizar las funciones \"GroupBy/Agg\" y \"Window Functions\" para transformar los datos y asi obtener datos organizados que nos muestren un ranking según el piloto**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 3.3 - Utilizar las funciones \"GroupBy/Agg\" y \"Window Functions\" para transformar los datos y asi obtener datos organizados que nos muestren un ranking según el team**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte B"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 3.4 - Crear tablas con Databricks SQL**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Crearemos MANAGED TABLES para los tres archivos obtenidos en la transformación** \n",
    "\n",
    "La base de datos **f1_presentation** se creará en la ruta **/mnt/formula1dl/presentation**\n",
    "\n",
    "1. Modificaremos la ruta de escritura de todos los notebooks, para escribir los datos transformados en su tabla correspondiente\n",
    "\n",
    "2. Debemos eliminar el directorio y archivo parquet creado previamente para todos los archivos, dado que, al guardarlo como tabla creará el directorio y archivo y además creará la tabla\n",
    "\n",
    "2. Utilizaremos **lenguaje python** para crear las tablas, por lo que, por defecto serán **MANAGED TABLES**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 4 - Análisis**\n",
    "_____\n",
    "\n",
    "1. Crear una tabla de la cual nos apoyaremos para realizar nuestro análisis\n",
    "\n",
    "2. Realizar un análisis de desempeño por piloto\n",
    "\n",
    "3. Realizar un análisis de desempeño por equipo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 5 - Patrón de diseño de carga de datos**\n",
    "_____"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Vamos a suponer que los datos nos llegarán de forma diaria:\n",
    "\n",
    "    *   El dia 21 de Marzo de 2021 nos llegará toda la historia de carreras hasta la **raceid** igual a **1047**\n",
    "    *   El dia 28 de Marzo de 2021 nos llegará solo la carrera de **raceid** igual a **1052**, que se realizó ese dia\n",
    "    *   El dia 18 de Abril de 2021 nos llegará solo la carrera de **raceid** igual a **1053**, que se realizó ese dia\n",
    "    *   Las carreras con **raceid** igual a **1049**, **1050** no se realizaron. La carrera de **raceid** igual a **1051** se postergó  \n",
    "    *   El próximo archivo nos llegará el día 2 de Mayo de 2021 y corresponde a la **raceid** igual a **1054**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/J7Bc7Pp7/db138.png\"></center>\n",
    "\n",
    "2. Vamos a analizar la data de los archivos que se encuentran en los tres directorios:\n",
    "\n",
    "    *   Primero, indicar que todos los dias que se haga una carrera nos llegará el archivo. Este será un directorio que llevará la fecha\n",
    "        de la carrera por nombre y contendrá 8 archivos:\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/pX507K94/db140.png\"></center>    \n",
    "\n",
    "3. Los archivos **circuits**, **races**, **constructors** y **drivers** son iguales para todos, es decir, la data que se encuentra en esos 4 archivos se repite en los 3 directorios **2021-03-21**, **2021-03-28** y **2021-04-18**. \n",
    "\n",
    "4. Los archivos **results**, **pit_stops** y los directorios con archivos **lap_times** y **qualifying**, solo traeran data correspondiente a\n",
    "la carrera en cuestión. Por ejemplo, para la fecha **2021-03-21** el archivo **results** traerá data correspondiente a toda la historia de carreras hasta la **raceid** igual a **1047**. No así para el archivo **2021-03-28** para el cual traerá solo data de la carrera realizada \n",
    "dicho dia. Lo mismo para el dia **2021-04-18**\n",
    "\n",
    "5. Podemos ver en el archivo **races** las carreras programadas desde el 29 de Marzo de 2009 hasta el 12 de Diciembre de 2021. Ahi podemos verificar las fechas en que irán realizando las proximas carreras. También comentar que la carrera de **raceid** igual a **1047** se realizó el 13 de Diciembre de 2020. Posteriormente, se comenzó a utilizar Azure y Databricks y los datos desde la primera carrera hasta la realizada en esa fecha, fue enviada el dia 21 de Marzo de 2021. A partir de la carrera de **raceid** igual a **1052** se fue enviando la data el dia que se realizó la carrera.\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/NfjNs12d/db139.png\"></center>\n",
    "\n",
    "6. Antes de crear un patrón de diseño para la carga de nuestros datos, teniamos el siguiente patrón de carga donde cada vez que llega un directorio nuevo cada dia VOLVEMOS A CARGAR TODO y SOBRESCRIBIMOS LO ANTERIOR:\n",
    "\n",
    "Nos llega el directorio **2021-03-21** y lo procesamos\n",
    "<center><img src=\"https://i.postimg.cc/fRjVTgtd/db141.png\"></center>\n",
    "\n",
    "Ya tenemos en nuestro repositorio **raw** el directorio **2021-03-21** y ahora nos llegó el directorio **2021-03-28**. Procesamos ambos, sobrescribiendo todo lo realizado en el proceso anterior, es decir, lo realizado para el directorio **2021-03-21**\n",
    "<center><img src=\"https://i.postimg.cc/GpX9v8jh/db142.png\"></center>\n",
    "\n",
    "Ya tenemos en nuestro repositorio **raw** el directorio **2021-03-21** y el directorio **2021-03-28** y ahora nos llegó el directorio **2021-04-18**. Procesamos los tres, sobrescribiendo todo lo realizado en el proceso anterior, es decir, lo realizado para el directorio **2021-03-21** y para el directorio **2021-03-28**\n",
    "<center><img src=\"https://i.postimg.cc/mDhkjLd3/db143.png\"></center>\n",
    "\n",
    "En resúmen, siempre volvemos a procesar TODO y NO SÓLO LOS DATOS NUEVOS QUE HAN LLEGADO. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 5.1 - Eliminar las Bases de datos y tablas de \"f1_proccesed\" y \"f1_presentation\" y volver a crearlas**\n",
    "\n",
    "1. Comenzaremos vaciando el contenedor **raw** y cargando los directorios de los 3 dias, **2021-03-21**, **2021-03-28** y **2021-04-18**. La ruta para cada directorio vendria a ser de la siguiente forma:\n",
    "\n",
    "    *   **/mnt/formula1dl/raw/2021-03-21**\n",
    "    *   **/mnt/formula1dl/raw/2021-03-28**\n",
    "    *   **/mnt/formula1dl/raw/2021-04-18**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/2jXCHXT7/db144.png\"></center>\n",
    "\n",
    "2. Eliminar las Bases de datos y tablas de **f1_proccesed** y **f1_presentation** y volver a crearlas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 5.2 - Método 1 de carga incremental utilizando el parámetro **p-file_date** del notebook**\n",
    "\n",
    "1. Carga única de los archivos que son iguales en todos los dias: **circuits**, **races**, **constructors** y **drivers**\n",
    "\n",
    "2. Modificar la **ruta de ingestión** utilizando el parámetro **p_file_date**\n",
    "\n",
    "3. Modificar el **modo** de escritura, cambiamos de **overwrite** a **append**\n",
    "\n",
    "3. Comenzar una carga incremental utilizando el parámetro **p-file_date** del notebook para los archivos **results** y **pit_stops** y los directorios con archivos **lap_times** y **qualifying**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 5.2 - Método 2 de carga incremental utilizando particiones**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 5.2 - Método 3 de carga incremental sobrescribiendo particiones**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 5.3 - Crear funciones y hacer más eficiente el código para el Método 3 de carga incremental**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 5.4 - Método de carga incremental para las transformaciones**\n",
    "\n",
    "1. Método de carga incremental para la transformación **race_results**. Esta transformación la encontramos en el Paso 3 --> Paso B --> Paso 3.4.1\n",
    "\n",
    "2. Método de carga incremental para la transformación **driver_standings**. Esta transformación la encontramos en el Paso 3 --> Paso B --> Paso 3.4.2\n",
    "\n",
    "3. Método de carga incremental para la transformación **constructor_standings**. Esta transformación la encontramos en el Paso 3 --> Paso B --> Paso 3.4.3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 6 - Crear una arquitectura utilizando Delta tables**\n",
    "_____"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 6.1 - Eliminar las Bases de datos y tablas de \"f1_proccesed\" y \"f1_presentation\" y volver a crearlas**\n",
    "\n",
    "1. Comenzaremos vaciando el contenedor **raw** y cargando los directorios de los 3 dias, **2021-03-21**, **2021-03-28** y **2021-04-18**. La ruta para cada directorio vendria a ser de la siguiente forma:\n",
    "\n",
    "    *   **/mnt/formula1dl/raw/2021-03-21**\n",
    "    *   **/mnt/formula1dl/raw/2021-03-28**\n",
    "    *   **/mnt/formula1dl/raw/2021-04-18**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/2jXCHXT7/db144.png\"></center>\n",
    "\n",
    "2. Eliminar las Bases de datos y tablas de **f1_proccesed** y **f1_presentation** y volver a crearlas\n",
    "\n",
    "3. Solo utilizaremos las bases de datos **f1_proccesed** y **f1_presentation** y sus tablas. No utilizaremos la base de datos **f1_raw** ni sus tablas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 6.2 - Carga única de los archivos que son iguales en todos los dias:** **circuits**, **races**, **constructors** y **drivers**\n",
    "\n",
    "- Reutilización del desarrollo de los archivos:\n",
    "\n",
    "    *   **Paso 5.2.1**\n",
    "    *   **Paso 5.2.2**\n",
    "    *   **Paso 5.2.3**\n",
    "    *   **Paso 5.2.4**\n",
    "```\n",
    "```\n",
    "- Modificaremos el formato a **delta** (o también podriamos no indicarlo y aun asi creariamos una **tabla delta**. Si indicamos que su formato corresponde a **parquet** estariamos creando una **tabla parquet** y no una tabla delta) al momento de crear la tabla.         "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 6.3 - Método de carga incremental para la ingestión**\n",
    "\n",
    "- Reutilización del desarrollo de los archivos:\n",
    "\n",
    "    *   **Paso 5.2.5**\n",
    "    *   **Paso 5.2.6**\n",
    "    *   **Paso 5.2.7**\n",
    "    *   **Paso 5.2.8**\n",
    "```\n",
    "```\n",
    "- Modificaremos nuestro método de carga incremental. Dado que estamos utilizando **tablas delta** tenemos la opción de utilizar un **MERGE** para ir insertando y/o actualizando nuestros datos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 6.4 - Método de carga incremental para las transformaciones**\n",
    "\n",
    "- Reutilización del desarrollo de los archivos:\n",
    "\n",
    "    *   **Paso 5.4.1**\n",
    "    *   **Paso 5.4.2**\n",
    "    *   **Paso 5.4.3**\n",
    "```\n",
    "```\n",
    "- Modificaremos nuestro método de carga incremental. Dado que estamos utilizando **tablas delta** tenemos la opción de utilizar un **MERGE** para ir insertando y/o actualizando nuestros datos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 6.5 - Método de carga incremental para tabla de análisis de resultados**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 7 - Orquestación utilizando Azure Data Factory**\n",
    "_____"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
