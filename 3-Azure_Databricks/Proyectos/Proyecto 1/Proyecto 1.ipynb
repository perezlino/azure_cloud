{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Proyecto 1 - Ingesta, Transformación, Carga y Reportería**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Arquitectura del Proyecto**\n",
    "\n",
    "<center><img src=\"\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 1 - Integración con ADLS**\n",
    "_____\n",
    "\n",
    "1. Pasos a seguir para realizar la integración con ADLS\n",
    "\n",
    "2. Codificar el montaje (mount) con ADLS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 2 - Data Ingestion**\n",
    "_____\n",
    "\n",
    "1. Ingesta manual de datos desde API hacia el contenedor **raw** en ADLS\n",
    "\n",
    "2. Pasos a seguir en nuestra Ingesta de archivos desde el contenedor **raw** hacia el contenedor **ingested** (ambos en ADLS)\n",
    "\n",
    "<center><img src=\"https://images2.imgbox.com/a1/9f/t0LTa2k6_o.png\"></center> <!--db61-->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte A"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 2.1 - Ingesta del archivo \"circuits.csv\"**\n",
    "\n",
    "1. Leer el archivo\n",
    "\n",
    "2. Seleccionar solo las columnas que necesitamos\n",
    "\n",
    "3. Cambiar el nombre de ciertas columnas\n",
    "\n",
    "4. Añadir la fecha de ingestión al dataframe\n",
    "\n",
    "5. Escribir datos en el contenedor **processed** del ADLS como **parquet**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/B6dF4Thd/db58.png\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 2.2 - Ingesta del archivo \"races.csv\"**\n",
    "\n",
    "1. Leer el archivo\n",
    "\n",
    "2. Añadir las columnas \"ingestion_date\" y \"race_timestamp\"\n",
    "\n",
    "3. Seleccionar sólo las columnas necesarias y renombrarlas como corresponda\n",
    "\n",
    "4. Escribir datos en el contenedor **processed** del ADLS como **parquet**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/PqNskYvb/db59.png\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 2.3 - Ingesta del archivo \"constructors.json\"**\n",
    "\n",
    "1. Leer el archivo\n",
    "\n",
    "2. Eliminar las columnas no deseadas\n",
    "\n",
    "3. Cambiar el nombre de las columnas y añadir \"ingestion date\"\n",
    "\n",
    "4. Escribir datos en el contenedor **processed** del ADLS como **parquet**\n",
    "\n",
    "<center><img src=\"https://images2.imgbox.com/44/b5/oUpevSA8_o.png\"></center> <!--db62-->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 2.4 - Ingesta del archivo \"drivers.json\"**\n",
    "\n",
    "1. Leer el archivo\n",
    "\n",
    "2. Renombrar columnas y añadir nuevas columnas\n",
    "\n",
    "3. Eliminar las columnas no deseadas\n",
    "\n",
    "4. Escribir datos en el contenedor **processed** del ADLS como **parquet**\n",
    "\n",
    "<center><img src=\"https://images2.imgbox.com/e4/f7/eGqClVvp_o.png\"></center> <!--db63-->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 2.5 - Ingesta del archivo \"results.json\"**\n",
    "\n",
    "1. Leer el archivo\n",
    "\n",
    "2. Renombrar columnas y añadir nuevas columnas\n",
    "\n",
    "3. Eliminar las columnas no deseadas\n",
    "\n",
    "4. Escribir datos en el contenedor **processed** del ADLS como **parquet**\n",
    "\n",
    "<center><img src=\"https://images2.imgbox.com/bd/ee/T5xzXTnY_o.png\"></center> <!--db64-->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 2.6 - Ingesta del archivo \"pit_stops.json\"**\n",
    "\n",
    "1. Leer el archivo\n",
    "\n",
    "2. Renombrar columnas y añadir nuevas columnas\n",
    "\n",
    "3. Escribir datos en el contenedor **processed** del ADLS como **parquet**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/dV8W94f3/db65.png\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 2.7 - Ingesta de los archivos \"lap_times_split.csv\"**\n",
    "\n",
    "1. Leer el directorio **lap_times** el cual contiene multiples archivos CSV\n",
    "\n",
    "2. Renombrar columnas y añadir nuevas columnas\n",
    "\n",
    "3. Escribir datos en el contenedor **processed** del ADLS como **parquet**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/ZntjGQ01/db66.png\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 2.8 - Ingesta de los archivos \"qualifying_split.json\"**\n",
    "\n",
    "1. Leer el directorio **qualifying** el cual contiene multiples archivos Multi Line JSON\n",
    "\n",
    "2. Renombrar columnas y añadir nuevas columnas\n",
    "\n",
    "3. Escribir datos en el contenedor **processed** del ADLS como **parquet**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/pV94qxzr/db67.png\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte B"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 2.9 - Databricks Workflows**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 2.9.1 - En primer lugar, veremos cómo incluir un notebook en otro notebook con parámetros de configuración y funciones comunes**\n",
    "\n",
    "En cualquier lenguaje de programación, es importante escribir código reutilizable para que un proyecto sea más manejable y Spark no es una excepción. Spark nos permite reutilizar un notebook en otro notebook usando el comando mágico **%run**. Y esto ya lo hemos visto anteriormente, pero queremos aplicarlo a nuestro proyecto para que nuestra comprensión sea más concreta."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 2.9.2 - Definir parámetros en un notebook**\n",
    "\n",
    "Vamos a ver cómo pasar parámetros a los notebooks de Databricks. En la última lección establecimos que el comando %run nos daba la capacidad de reutilizar código. De manera similar, los parámetros nos ayudan a utilizar el notebook completo. Por ejemplo, si estás procesando datos de dos fuentes diferentes con las mismas características, en lugar de escribir dos notebooks diferentes, puedes escribir un notebook y enviar el nombre de la fuente de datos como parámetro. En nuestro proyecto, vamos a suponer que podemos obtener los datos de la \"API eargast\", como hemos comentado antes, o de la página web oficial de la Fórmula Uno directamente. Queremos procesar ambos conjuntos de datos utilizando los mismos notebooks, pero queremos almacenar el nombre de la fuente de datos junto a los datos para poder identificar el origen de los datos para cualquier propósito explicativo. En este caso, podemos parametrizar todos nuestros notebooks con el nombre de la fuente de datos como parámetro y enviar el valor en tiempo de ejecución. Los \"widgets\" de Databricks nos ayudan a hacerlo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 2.9.3 - Invocar un notebook desde otro y encadenarlos y pasarles también los parámetros que hemos definido antes**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 2.9.4 - Crear databricks jobs para ejecutar notebooks a intervalos regulares y bajo demanda**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al final de este proceso, se espera lograr una buena comprensión de las capacidades que ofrece Databricks para crear notebook workflows y ejecutarlos desde databricks jobs."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
