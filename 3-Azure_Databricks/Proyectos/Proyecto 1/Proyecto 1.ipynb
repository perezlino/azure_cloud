{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Proyecto 1 - Ingesta, Transformación, Carga y Reportería**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Arquitectura del Proyecto**\n",
    "\n",
    "<center><img src=\"\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 1 - Integración con ADLS**\n",
    "_____\n",
    "\n",
    "1. Pasos a seguir para realizar la integración con ADLS\n",
    "\n",
    "2. Codificar el montaje (mount) con ADLS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 2 - Data Ingestion**\n",
    "_____\n",
    "\n",
    "1. Ingesta manual de datos desde API hacia el contenedor **raw** en ADLS\n",
    "\n",
    "2. Pasos a seguir en nuestra Ingesta de archivos desde el contenedor **raw** hacia el contenedor **ingested** (ambos en ADLS)\n",
    "\n",
    "<center><img src=\"https://images2.imgbox.com/a1/9f/t0LTa2k6_o.png\"></center> <!--db61-->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte A"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 2.1 - Ingesta del archivo \"circuits.csv\"**\n",
    "\n",
    "1. Leer el archivo\n",
    "\n",
    "2. Seleccionar solo las columnas que necesitamos\n",
    "\n",
    "3. Cambiar el nombre de ciertas columnas\n",
    "\n",
    "4. Añadir la fecha de ingestión al dataframe\n",
    "\n",
    "5. Escribir datos en el contenedor **processed** del ADLS como **parquet**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/B6dF4Thd/db58.png\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 2.2 - Ingesta del archivo \"races.csv\"**\n",
    "\n",
    "1. Leer el archivo\n",
    "\n",
    "2. Añadir las columnas \"ingestion_date\" y \"race_timestamp\"\n",
    "\n",
    "3. Seleccionar sólo las columnas necesarias y renombrarlas como corresponda\n",
    "\n",
    "4. Escribir datos en el contenedor **processed** del ADLS como **parquet**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/PqNskYvb/db59.png\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 2.3 - Ingesta del archivo \"constructors.json\"**\n",
    "\n",
    "1. Leer el archivo\n",
    "\n",
    "2. Eliminar las columnas no deseadas\n",
    "\n",
    "3. Cambiar el nombre de las columnas y añadir \"ingestion date\"\n",
    "\n",
    "4. Escribir datos en el contenedor **processed** del ADLS como **parquet**\n",
    "\n",
    "<center><img src=\"https://images2.imgbox.com/44/b5/oUpevSA8_o.png\"></center> <!--db62-->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 2.4 - Ingesta del archivo \"drivers.json\"**\n",
    "\n",
    "1. Leer el archivo\n",
    "\n",
    "2. Renombrar columnas y añadir nuevas columnas\n",
    "\n",
    "3. Eliminar las columnas no deseadas\n",
    "\n",
    "4. Escribir datos en el contenedor **processed** del ADLS como **parquet**\n",
    "\n",
    "<center><img src=\"https://images2.imgbox.com/e4/f7/eGqClVvp_o.png\"></center> <!--db63-->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 2.5 - Ingesta del archivo \"results.json\"**\n",
    "\n",
    "1. Leer el archivo\n",
    "\n",
    "2. Renombrar columnas y añadir nuevas columnas\n",
    "\n",
    "3. Eliminar las columnas no deseadas\n",
    "\n",
    "4. Escribir datos en el contenedor **processed** del ADLS como **parquet**\n",
    "\n",
    "<center><img src=\"https://images2.imgbox.com/bd/ee/T5xzXTnY_o.png\"></center> <!--db64-->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 2.6 - Ingesta del archivo \"pit_stops.json\"**\n",
    "\n",
    "1. Leer el archivo\n",
    "\n",
    "2. Renombrar columnas y añadir nuevas columnas\n",
    "\n",
    "3. Escribir datos en el contenedor **processed** del ADLS como **parquet**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/dV8W94f3/db65.png\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 2.7 - Ingesta de los archivos \"lap_times_split.csv\"**\n",
    "\n",
    "1. Leer el directorio **lap_times** el cual contiene multiples archivos CSV\n",
    "\n",
    "2. Renombrar columnas y añadir nuevas columnas\n",
    "\n",
    "3. Escribir datos en el contenedor **processed** del ADLS como **parquet**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/ZntjGQ01/db66.png\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 2.8 - Ingesta de los archivos \"qualifying_split.json\"**\n",
    "\n",
    "1. Leer el directorio **qualifying** el cual contiene multiples archivos Multi Line JSON\n",
    "\n",
    "2. Renombrar columnas y añadir nuevas columnas\n",
    "\n",
    "3. Escribir datos en el contenedor **processed** del ADLS como **parquet**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/pV94qxzr/db67.png\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte B"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 2.9 - Databricks Workflows**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 2.9.1 - En primer lugar, veremos cómo incluir un notebook en otro notebook con parámetros de configuración y funciones comunes**\n",
    "\n",
    "En cualquier lenguaje de programación, es importante escribir código reutilizable para que un proyecto sea más manejable y Spark no es una excepción. Spark nos permite reutilizar un notebook en otro notebook usando el comando mágico **%run**. Y esto ya lo hemos visto anteriormente, pero queremos aplicarlo a nuestro proyecto para que nuestra comprensión sea más concreta."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 2.9.2 - Definir parámetros en un notebook**\n",
    "\n",
    "Vamos a ver cómo pasar parámetros a los notebooks de Databricks. En el punto 2.9.1 establecimos que el comando **%run** nos daba la capacidad de reutilizar código. Por ejemplo, si estás procesando datos de dos fuentes diferentes con las mismas características, en lugar de escribir dos notebooks diferentes, puedes escribir un notebook y enviar el nombre de la fuente de datos como parámetro. En nuestro proyecto, vamos a suponer que podemos obtener los datos de la \"API eargast\", como hemos comentado antes, o de la página web oficial de la Fórmula Uno directamente. Queremos procesar ambos conjuntos de datos utilizando los mismos notebooks, pero queremos almacenar el nombre de la fuente de datos junto a los datos para poder identificar el origen de los datos para cualquier propósito explicativo. En este caso, podemos parametrizar todos nuestros notebooks con el nombre de la fuente de datos como parámetro y enviar el valor en tiempo de ejecución. Los \"widgets\" de Databricks nos ayudan a hacerlo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 2.9.3 - Invocar un notebook desde otro y encadenarlos y pasarles también los parámetros que hemos definido antes**\n",
    "\n",
    "Ahora que tenemos ocho notebooks diferentes y que hasta ahora los hemos estado ejecutando a mano, puede que te estés preguntando cómo se ejecutan en escenarios de producción. Databricks nos brinda jobs que puedes programar para que se ejecuten a una hora específica, a un intervalo regular, lo que está muy bien, pero carece de muchas funcionalidades que normalmente obtendrías con cualquier tipo de herramientas de programación. Por ejemplo, no puedes crear dependencias entre jobs. Para solucionar eso, Databricks nos brinda una utilidad, llamada **Notebook Utility**, que te permite crear notebook workflow. Dicho esto, si estás en un Azure, no es probable que estés usando Databricks para crear workflows y programar jobs (scheduling jobs) en producción, debido a su funcionalidad inferior. Azure nos brinda una herramienta de workflow mucho mejor llamada **Azure Data Factory**, y eso es lo que utilizaremos. También me gustaría llevarte a través del notebook workflow y Databricks jobs para que puedas crear jobs rápidos para testear workloads sin tener que usar Azure Data Factory. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 2.9.4 - Crear databricks jobs para ejecutar notebooks a intervalos regulares y bajo demanda**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al final de este proceso, se espera lograr una buena comprensión de las capacidades que ofrece Databricks para crear notebook workflows y ejecutarlos desde databricks jobs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte C"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 2.10 - Crear tablas con Spark SQL**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 2.10.1 - Crearemos EXTERNAL TABLES para todos los archivos que se encuentren en el directorio \"raw\" del ADLS** \n",
    "\n",
    "La base de datos **f1_raw** se creará por defecto en la ruta **/user/hive/warehouse/f1_raw.db**\n",
    "\n",
    "1. Una **external table** que haga referencia al archivo **circuits.csv** ubicada en la ruta **/mnt/formula1dl/raw/circuits.csv**\n",
    "\n",
    "2. Una **external table** que haga referencia al archivo **races.csv** ubicada en la ruta **/mnt/formula1dl/raw/races.csv**\n",
    "\n",
    "3. Una **external table** que haga referencia al archivo **constructors.json** ubicada en la ruta **/mnt/formula1dl/raw/constructors.json**\n",
    "\n",
    "4. Una **external table** que haga referencia al archivo **drivers.json** ubicada en la ruta **/mnt/formula1dl/raw/drivers.json**\n",
    "\n",
    "5. Una **external table** que haga referencia al archivo **results.json** ubicada en la ruta **/mnt/formula1dl/raw/results.json**\n",
    "\n",
    "6. Una **external table** que haga referencia al archivo **pit_stops.json** ubicada en la ruta **/mnt/formula1dl/raw/pit_stops.json**\n",
    "\n",
    "7. Una **external table** que haga referencia al directorio **lap_times** ubicada en la ruta **/mnt/formula1dl/raw/lap_times**\n",
    "\n",
    "8. Una **external table** que haga referencia al directorio **qualifying** ubicada en la ruta **/mnt/formula1dl/raw/qualifying**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 2.10.2 - Crearemos MANAGED TABLES para todos los archivos ingestados y modificados**\n",
    "\n",
    "La base de datos **f1_processed** se creará en la ruta **/mnt/formula1dl/processed**\n",
    "\n",
    "1. Modificaremos la ruta de escritura de todos los notebooks, para escribir los datos ingestados en su tabla correspondiente\n",
    "\n",
    "2. Debemos eliminar el directorio y archivo parquet creado previamente para todos los archivos, dado que, al guardarlo como tabla creará el directorio y archivo y además creará la tabla\n",
    "\n",
    "2. Utilizaremos **lenguaje python** para crear las tablas, por lo que, por defecto serán **MANAGED TABLES**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 3 - Data Transformation**\n",
    "_____\n",
    "\n",
    "1. Modificar nombres de columnas que no realizamos en la etapa de **Data Ingestion**\n",
    "\n",
    "2. Utilizar las funciones **Filter** y **Join** para transformar los datos\n",
    "```\n",
    "    Utilizaremos la función \"Join\" sobre estos 5 dataframes\n",
    "```    \n",
    "<center><img src=\"https://i.postimg.cc/tC7fWR4B/db85.png\"></center>\n",
    "\n",
    "```\n",
    "    Utilizaremos los siguientes campos\n",
    "```\n",
    "<center><img src=\"https://i.postimg.cc/7YRzX2FJ/db86.png\"></center>\n",
    "\n",
    "3. Utilizar las funciones **GroupBy** y **Agg** para transformar los datos\n",
    "\n",
    "4. Utilizar las **Window functions** para transformar los datos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte A"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 3.1 - Utilizar las funciones \"Filter\" y \"Join\" para transformar los datos y asi obtener datos organizados que nos muestren los resultados de las carreras**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 3.2 - Utilizar las funciones \"GroupBy/Agg\" y \"Window Functions\" para transformar los datos y asi obtener datos organizados que nos muestren un ranking según el piloto**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 3.3 - Utilizar las funciones \"GroupBy/Agg\" y \"Window Functions\" para transformar los datos y asi obtener datos organizados que nos muestren un ranking según el team**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte B"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Paso 3.4 - Crear tablas con Spark SQL**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Crearemos MANAGED TABLES para los tres archivos obtenidos en la transformación** \n",
    "\n",
    "La base de datos **f1_presentation** se creará en la ruta **/mnt/formula1dl/presentation**\n",
    "\n",
    "1. Modificaremos la ruta de escritura de todos los notebooks, para escribir los datos transformados en su tabla correspondiente\n",
    "\n",
    "2. Debemos eliminar el directorio y archivo parquet creado previamente para todos los archivos, dado que, al guardarlo como tabla creará el directorio y archivo y además creará la tabla\n",
    "\n",
    "2. Utilizaremos **lenguaje python** para crear las tablas, por lo que, por defecto serán **MANAGED TABLES**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
