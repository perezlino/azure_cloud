{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"d3074eb6-c62e-4d9f-bed5-ec4902fa6308","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>"]},"metadata":{"application/vnd.databricks.v1+output":{"arguments":{},"data":"","errorSummary":"","errorTraceType":null,"metadata":{},"type":"ipynbError"}},"output_type":"display_data"}],"source":["from pyspark.sql.functions import current_timestamp\n","def add_ingestion_date(input_df):\n","  output_df = input_df.withColumn(\"ingestion_date\", current_timestamp())\n","  return output_df"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"59998c66-7fe5-4933-8f54-79e0041a4ee0","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>"]},"metadata":{"application/vnd.databricks.v1+output":{"arguments":{},"data":"","errorSummary":"","errorTraceType":null,"metadata":{},"type":"ipynbError"}},"output_type":"display_data"}],"source":["def re_arrange_partition_column(input_df, partition_column):\n","  column_list = []\n","  for column_name in input_df.schema.names:\n","    if column_name != partition_column:\n","      column_list.append(column_name)\n","  column_list.append(partition_column)\n","  output_df = input_df.select(column_list)\n","  return output_df"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"c502da21-d472-4323-9573-47a0adbc5826","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>"]},"metadata":{"application/vnd.databricks.v1+output":{"arguments":{},"data":"","errorSummary":"","errorTraceType":null,"metadata":{},"type":"ipynbError"}},"output_type":"display_data"}],"source":["def overwrite_partition(input_df, db_name, table_name, partition_column):\n","  output_df = re_arrange_partition_column(input_df, partition_column)\n","  spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\",\"dynamic\")\n","  if (spark._jsparkSession.catalog().tableExists(f\"{db_name}.{table_name}\")):\n","    output_df.write.mode(\"overwrite\").insertInto(f\"{db_name}.{table_name}\")\n","  else:\n","    output_df.write.mode(\"overwrite\").partitionBy(partition_column).format(\"parquet\").saveAsTable(f\"{db_name}.{table_name}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"884fad05-1b52-4cb9-853e-c168cad2c691","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>"]},"metadata":{"application/vnd.databricks.v1+output":{"arguments":{},"data":"","errorSummary":"","errorTraceType":null,"metadata":{},"type":"ipynbError"}},"output_type":"display_data"}],"source":["def df_column_to_list(input_df, column_name):\n","  df_row_list = input_df.select(column_name) \\\n","                        .distinct() \\\n","                        .collect()\n","  \n","  column_value_list = [row[column_name] for row in df_row_list]\n","  return column_value_list"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":2},"notebookName":"common_functions","widgets":{}},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
