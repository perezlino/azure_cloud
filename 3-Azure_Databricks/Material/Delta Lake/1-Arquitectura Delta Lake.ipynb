{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) **Arquitectura Lakehouse (Delta Lake)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data warehouses**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los Data Warehouse surgieron en la década de 1980, cuando las empresas quisieron tomar decisiones basadas en todos los datos disponibles en una organización, en un solo lugar, en lugar de buscar datos en departamentos individuales. Un Data Warehouse estaba formado principalmente por datos operativos disponibles dentro de la organización. Algunos grandes Data Warehouse también recopilaban datos externos para tomar decisiones inteligentes. Los datos recibidos en un Data warehouse están principalmente estructurados como tablas SQL o archivos CSV o semiestructurados como archivos Json o XML. No tienen capacidad para procesar datos no estructurados. A continuación, los datos recibidos pasan por el proceso ETL (Extract, Transform and Load) y luego se cargan en un Data warehouse o Data marts. Los datos cargados en los Data marts, se limpian, validan y aumentan con el significado de negocio, también, son generalmente altamente agregados para proporcionar un valor de negocio significativo, como los indicadores clave de procesamiento o KPS. A continuación, los analistas y los responsables de la empresa consumen estos datos a través de informes de BI. Eran muy valiosos para tomar decisiones de negocio y la mayoría de las grandes empresas tenían al menos un Data warehouse a principios de la década de 2000. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://i.postimg.cc/7Yd0kTNf/db163.png\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al mismo tiempo, tenían algunos problemas importantes. A principios de la década de 2000, debido a la popularidad de Internet, el volumen de datos empezó a aumentar significativamente y también la variedad de sus datos había empezado a cambiar. Empezamos a ver datos no estructurados como vídeos, imágenes, archivos de texto, etc., y son muy, muy valiosos para tomar decisiones. Pero los Data warehouses carecían de soporte para los datos no estructurados que estábamos viendo. En un Data warehouse, los datos se cargaban una vez comprobada su calidad y también una vez transformados. Esto significaba que se tardaba más en desarrollar una solución para introducir nuevos datos en el Data Warehouse. Los Data Warehouse se construían sobre bases de datos relacionales tradicionales o soluciones EMP, lo que significaba que utilizaban formatos de archivo propietarios y daban lugar a inicios de sesión de proveedores. También los Data Warehouse \"on-prem\" tradicionales eran muy, muy difíciles de escalar o, a veces, imposible. Esto significaba que se necesitaban grandes proyectos de migración de datos para escalar esas bases de datos. El almacenamiento era caro con los grandes proveedores y tampoco era posible escalar el almacenamiento sin computación, etc. Por último, los Data Warehouse no ofrecían suficiente soporte para las cargas de trabajo de Data Science o ML/AI."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data Lakes**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La arquitectura Data lake tenía como objetivo resolver los problemas que hemos comentado sobre los Data warehouses. Surgieron alrededor del año 2011. Los Data lakes no sólo pueden manejar datos estructurados y semiestructurados, sino también datos no estructurados, que son aproximadamente el 90 por ciento de los datos que vemos actualmente. Los datos recibidos se introducen en un Data Lake sin ningún tipo de limpieza o transformación. De este modo, se agilizan los plazos de desarrollo de soluciones y los tiempos de ingestión. HDFS y los cloud objects stores, como Amazon S3 o Azure Data Lake, son realmente baratos. Así que las organizaciones podían ingerir todos los datos sin preocuparse demasiado por el coste, lo cual es estupendo. Los Data lakes se construyeron sobre formatos de archivo de código abierto como Parquet, ORC o Avro, lo que significaba que podíamos utilizar una amplia variedad de software y bibliotecas para procesar y analizar los datos. Data science y Machine learning workloads podían utilizar los datos en bruto (raw data), así como los datos transformados en el Data lake. Pero había un problema importante, los Data lakes eran demasiado lentos para dar servicio a informes de BI interactivos, y había falta de gobernanza para los datos. Así que la industria pasó a copiar de nuevo el subconjunto de datos del Data lake a los Data warehouses para dar soporte a estos informes. El resultado fue una arquitectura torpe con demasiadas partes móviles."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://i.postimg.cc/5yxwYqF3/db162.png\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, resumamos los problemas de un Data lake. Data lake no ofrecía soporte para transacciones ACID, eso costaba una serie de problemas, los jobs fallidos dejaban archivos parcialmente cargados en el Data lake, y tenían que ser limpiados en procesos separados durante cada re-ejecución. No había garantía de lecturas consistentes, los usuarios podían estar leyendo datos parcialmente escritos, resultando en un Data lake poco fiable. Gestionar una corrección de los datos enviados era una pesadilla porque los Data lakes no ofrecían soporte para updates, por lo que los desarrolladores tenían que particionar los datos y reescribir toda la partición en estas circunstancias, lo que se traducía en un aumento del tiempo de desarrollo. Los Data Lakes no ofrecían soporte para revertir (roll back) los datos que se estaban escribiendo, por lo que, se tenía que reescribir una partición o reescribir una tabla entera. Como parte del GDPR (\"General Data Protection Regulation\" de la Unión Europea) los usuarios tenían el derecho a ser olvidados, para implementar esto, las compañías tenían que borrar esos datos individuales del Data lake. Los Data lakes no ofrecen un historial de versiones de los datos, lo que dificulta los rollbacks, así como la gobernanza. La combinación de todos estos factores dio lugar a un pantano de datos poco fiables. Además de esto, los Data lakes ofrecían un rendimiento deficiente para las consultas interactivas y también un soporte de BI deficiente en términos de rendimiento, así como de seguridad, gobernanza, etc., y como hemos visto hasta ahora, muy complejos de configurar. Por último, necesitábamos procesar los datos en streaming de forma separada a los datos en batch, lo que daba lugar a complejas arquitecturas lambda. Un punto que no está aquí. No he mostrado esto en la arquitectura. Los diagramas de streaming están fuera del alcance de este curso. En resumen, los Data Warehouses, son muy buenos para lidiar con grandes cargas de trabajo, pero carecían de soporte para streaming, data science y machine learning workloads. Los Data lakes, por otro lado, se centraban principalmente en las cargas de trabajo de Data science y Machine learning. Pero no llegaban a satisfacer las cargas de trabajo de BI tradicionales y admitían cargas de trabajo de streaming, pero resultaba difícil combinar cargas de trabajo de streaming y batch. Además de esto, debido a la falta de soporte para transacciones ACID, acabábamos teniendo pantanos de datos poco fiables en nuestros Data lakes. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data Lakehouse**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de la arquitectura Lakehouse es ofrecer lo mejor tanto de los Data Warehouse como de los Delta Lakes. Se han diseñado para proporcionar un mejor soporte de IA, así como de Data science y Machine learning. Veamos cómo es una arquitectura Data Lakehouse. De forma similar a los Data lakes, podemos ingerir los datos operativos y externos en Delta Lakes. Delta Lake no es más que un Data lake con controles de transacciones ACID. Debido a la capacidad de tener transacciones ACID, ahora también podemos combinar streaming y batch workloads, y eliminar la necesidad de una arquitectura Lambda. Estos datos podrían entonces ser transformados más eficientemente sin la necesidad de reescribir particiones enteras en casos de reprocesamiento de datos o reescribir Data lakes en caso de procesar solicitudes GDPR. Por lo general, hay más de una etapa de transformación, pero he mostrado una sola aquí solo por simplicidad. La arquitectura de Databricks tiene dos etapas, con la primera etapa transformando los datos en bruto (raw data) para proporcionar una estructura, realizar cualquier limpieza y calidad de datos, etc. y devolver a un conjunto de tablas que se llaman **Tablas Silver**. Y las tablas en bruto (raw tables) se llaman **Tablas Bronze**. Así que se pasa de **Bronze** a **Silver** y luego los datos de las **Tablas Silver** se agregan para proporcionar cuadros de negocio y se llaman **Tablas Gold**. Para simplificar, como ya he dicho, he utilizado aquí un solo cuadro. Los datos de cualquiera de estos conjuntos de tablas Delta podría entonces ser utilizado para la ciencia de datos y machine learning workloads, Delta Lakes también proporciona conectores a herramientas de BI como Power BI y Tableau. Además, podemos tener roles en los controles de acceso definidos para la gobernanza de datos. Esto elimina la necesidad de copiar los datos a un Data Warehouse. Así que esto es básicamente la arquitectura Lakehouse, pero todavía hay proyectos que ejecutan informes de BI desde una base de datos relacional en lugar del Delta Lake debido a la falta de rendimiento. Delta lake existe desde 2018 y evoluciona rápidamente. Pero el rendimiento aún no se puede comparar con un Data Warehouse cuando se trata de informes. Así que si buscas ese nivel de rendimiento, que obtienes de un Data Warehouse, deberás seguir copiando los datos a un Data Warehouse. Pero esa es la situación actual. Y creo que dentro de unos años los Delta Lakes ofrecerán un rendimiento comparable y no tendremos que recurrir a los Data Warehouse."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://i.postimg.cc/5NkhnH0p/db161.png\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a resumir rápidamente los beneficios de pasar a la arquitectura Lakehouse. Al igual que los Data Lake, los Delta Lake manejan todo tipo de datos y siguen funcionando en Cloud Object Store, como S3 y ADLS. Así que tenemos beneficios de costes allí y utilizan formatos de archivo de código abierto y Delta lake utiliza Parquet. Admiten todo tipo de cargas de trabajo, como BI, Data science y Machine learning. Ofrecen la posibilidad de utilizar herramientas de BI directamente en ellos. Y lo que es más importante, proporcionan historial de soporte ACID y versionado. Esto nos ayuda a detener el pantano de datos poco fiables que se crean como en el caso de Data lake. Proporcionan un mejor rendimiento en comparación con los Data lakes y por último proporcionan una arquitectura muy sencilla, no necesitamos la arquitectura Lambda para streaming y batch workloads y además potencialmente podríamos eliminar la necesidad de copiar los datos del Data lake al Data warehouse. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que entendemos los beneficios de la arquitectura Lakehouse, profundicemos un poco más y veamos cómo Delta Lake proporciona las capacidades que acabamos de discutir. Al igual que un Data Lake, Delta Lake también almacena los datos como archivos Parquet, que es un formato de código abierto. La diferencia clave aquí es que al almacenar los datos en Parquet, Delta Lake también crea una transacción log junto a ese archivo. Esto es lo que proporciona la historia, el versionado, el soporte de transacciones ACID, el viaje en el tiempo, etc. Esta es una diferencia clave entre un Data lake y un Delta lake. La siguiente capa es el motor Delta, que es un motor de consultas compatible con Spark y optimizado para el rendimiento de los Delta Lakes. También utiliza Spark para la gestión a través de las transacciones log para que pueda ser distribuido también. Luego tenemos las tablas Delta, que podemos registrar con Hive Metastore para proporcionar seguridad mediante nuestros roles y accesos, gobernanza, integridad de datos mediante nuestras restricciones, así como mejores optimizaciones. En las tablas del Delta Lake, podemos ejecutar Spark workloads como cualquier tipo de transformaciones SQL, machine learning workloads y streaming workloads, como hacemos en un Data Lake. Y lo que es más importante, ahora las cargas de trabajo de BI pueden acceder directamente al Delta lake. Puede parecer que hay mucho que hacer aquí, pero como desarrollador, es muy sencillo pasar de utilizar un archivo Parquet en un Data lake a Delta lake. Simplemente tienes que cambiar el formato de **parquet** a **delta** en spark.read o cuando escribes los datos en tus dataframe writer APIs. Los motores Spark y Delta se encargan del resto por ti."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
