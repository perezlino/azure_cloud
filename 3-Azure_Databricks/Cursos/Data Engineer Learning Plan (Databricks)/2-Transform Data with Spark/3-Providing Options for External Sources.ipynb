{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"cbb18b10-2500-48b7-bfe1-43622d2b2e0e","showTitle":false,"title":""}},"source":["<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n","  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n","</div>"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"7222c947-7cf4-40a8-864c-5144a189945e","showTitle":false,"title":""}},"source":["### **Opciones para fuentes externas**\n","Mientras que la consulta directa de archivos funciona bien para formatos autodescriptivos, muchas fuentes de datos requieren configuraciones adicionales o la declaración de schema para ingerir correctamente los registros.\n","\n","En esta lección, crearemos tablas utilizando fuentes de datos externas. Si bien estas tablas aún no se almacenarán en el formato de Delta Lake (y, por lo tanto, no estarán optimizadas para Lakehouse), esta técnica ayuda a facilitar la extracción de datos de diversos sistemas externos.\n","\n","#### **Objetivos de aprendizaje**\n","Al final de esta lección, deberías ser capaz de:\n","- Utilizar Spark SQL para configurar las opciones de extracción de datos de fuentes externas\n","- Crear tablas contra fuentes de datos externas para diversos formatos de archivo\n","- Describir el comportamiento predeterminado al consultar tablas definidas contra fuentes externas"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"424b59cb-fdd1-495d-98ea-8a115b8fb975","showTitle":false,"title":""}},"source":["#### **Run Setup**\n","\n","El setup script creará los datos y declarará los valores necesarios para que el resto de este notebook se ejecute."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"d7d008ed-5616-4b50-8fc5-d47d46e8bb07","showTitle":false,"title":""}},"outputs":[],"source":["%run ../Includes/Classroom-Setup-02.2"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"c91c1166-6fd3-493b-a7b8-ef15eeaf61a3","showTitle":false,"title":""}},"source":["#### **Cuando las consultas directas no funcionan**\n","\n","Aunque las vistas pueden utilizarse para realizar consultas directas a archivos entre sesiones, su utilidad es limitada.\n","\n","Los archivos CSV son uno de los formatos de archivo más comunes, pero una consulta directa a estos archivos rara vez devuelve los resultados deseados."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"9f24f753-1022-4ee5-a478-0b4476b68c88","showTitle":false,"title":""}},"outputs":[],"source":["%sql\n","SELECT * FROM csv.`${DA.paths.sales_csv}`"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<center><img src=\"https://i.postimg.cc/4xSLxK5r/db347.png\"></center>"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"87b7529e-cd4c-47b5-8c46-5926716400ca","showTitle":false,"title":""}},"source":["De lo anterior se desprende que:\n","1. La fila de cabecera se está extrayendo como una fila de tabla\n","1. Todas las columnas se cargan como una sola columna\n","1. El archivo está delimitado por pipes (**`|`**)\n","1. La última columna parece contener datos anidados que se truncan."]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"6ac653ea-ded9-4da9-ba73-1708f51c1a42","showTitle":false,"title":""}},"source":["#### **Registro de tablas en datos externos con opciones de lectura**\n","\n","Mientras que Spark extraerá algunas fuentes de datos autodescriptivas de forma eficiente utilizando la configuración por defecto, muchos formatos requerirán la declaración del schema u otras opciones.\n","\n","Aunque hay muchas <a href=\"https://docs.databricks.com/spark/latest/spark-sql/language-manual/sql-ref-syntax-ddl-create-table-using.html\" target=\"_blank\">configuraciones adicionales</a> que puede establecer al crear tablas contra fuentes externas, la sintaxis que se muestra a continuación demuestra lo esencial necesario para extraer datos de la mayoría de los formatos.\n","\n","<strong><code>\n","CREATE TABLE table_identifier (col_name1 col_type1, ...)<br/>\n","USING data_source<br/>\n","OPTIONS (key1 = val1, key2 = val2, ...)<br/>\n","LOCATION = path<br/>\n","</code></strong>\n","\n","Tenga en cuenta que las opciones se pasan con las keys como texto sin entrecomillar y los valores entre comillas. Spark soporta muchas <a href=\"https://docs.databricks.com/data/data-sources/index.html\" target=\"_blank\">fuentes de datos</a> con opciones personalizadas, y sistemas adicionales pueden tener soporte no oficial a través de <a href=\"https://docs.databricks.com/libraries/index.html\" target=\"_blank\">librerías externas</a>. \n","\n","**NOTA**: Dependiendo de la configuración de su workspace, es posible que necesite ayuda del administrador para cargar librerías y configurar los parámetros de seguridad necesarios para algunas fuentes de datos."]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"da38530d-91b7-444b-9ee8-786d2b72083b","showTitle":false,"title":""}},"source":["La celda a continuación demuestra el uso de Spark SQL DDL para crear una tabla contra una fuente CSV externa, especificando:\n","1. Los nombres y tipos de columnas\n","1. El formato del archivo\n","1. El delimitador utilizado para separar los campos\n","1. La presencia de una cabecera\n","1. La ruta donde se almacenan los datos"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"c683d9db-eebe-429e-b4e1-b94e6585b728","showTitle":false,"title":""}},"outputs":[],"source":["%sql\n","CREATE TABLE IF NOT EXISTS sales_csv (\n","  order_id LONG, \n","  email STRING, \n","  transactions_timestamp LONG, \n","  total_item_quantity INTEGER, \n","  purchase_revenue_in_usd DOUBLE, \n","  unique_items INTEGER, \n","  items STRING)\n","USING CSV\n","OPTIONS (\n","  header = \"true\",\n","  delimiter = \"|\"\n",")\n","LOCATION \"${DA.paths.sales_csv}\""]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"2e036940-6e55-4edd-9ea9-1c45d9762fff","showTitle":false,"title":""}},"source":["**NOTA:** Para crear una tabla contra una fuente externa en PySpark, puedes envolver este código SQL con la función **`spark.sql()`**."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"f74e4c37-d245-40c0-9218-34d5d6803abe","showTitle":false,"title":""}},"outputs":[],"source":["%python\n","spark.sql(f\"\"\"\n","CREATE TABLE IF NOT EXISTS sales_csv (\n","  order_id LONG, \n","  email STRING, \n","  transactions_timestamp LONG, \n","  total_item_quantity INTEGER, \n","  purchase_revenue_in_usd DOUBLE, \n","  unique_items INTEGER, \n","  items STRING)\n","USING CSV\n","OPTIONS (\n","  header = \"true\",\n","  delimiter = \"|\"\n",")\n","LOCATION \"{DA.paths.sales_csv}\"\n","\"\"\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"77e242fa-8f8c-4375-ba67-70d1c814b92f","showTitle":false,"title":""}},"source":["Observe que no se ha movido ningún dato durante la declaración de la tabla. \n","\n","Al igual que cuando consultamos directamente nuestros archivos y creamos una vista, seguimos apuntando a archivos almacenados en una ubicación externa.\n","\n","Ejecute la siguiente celda para confirmar que los datos se están cargando correctamente."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"42b31762-58fb-45fd-a8aa-ccc8b067ea78","showTitle":false,"title":""}},"outputs":[],"source":["%sql\n","SELECT * FROM sales_csv"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<center><img src=\"https://i.postimg.cc/JnWQsjBd/db348.png\"></center>"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"d9c53e99-675f-466f-8a35-958f2b7ba167","showTitle":false,"title":""}},"outputs":[],"source":["%sql\n","SELECT COUNT(*) FROM sales_csv"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<center><img src=\"https://i.postimg.cc/Rh4GDwSn/db349.png\"></center>"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"ec0174c1-d266-41e2-ae5e-b3440e8e2f4f","showTitle":false,"title":""}},"source":["Todos los metadatos y opciones pasados durante la declaración de la tabla se persistirán en el metastore, asegurando que los datos en la ubicación siempre se leerán con estas opciones.\n","\n","**NOTA**: Cuando se trabaja con CSV como fuente de datos, es importante asegurarse de que el orden de las columnas no cambia si se añaden archivos de datos adicionales al directorio de origen. Debido a que el formato de datos no tiene una fuerte aplicación del schema, Spark cargará las columnas y aplicará los nombres de las columnas y los tipos de datos en el orden especificado durante la declaración de la tabla.\n","\n","Al ejecutar **`DESCRIBE EXTENDED`** en una tabla se mostrarán todos los metadatos asociados a la definición de la tabla."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"5302a77d-c1de-49cf-bdcf-c83368db3f70","showTitle":false,"title":""}},"outputs":[],"source":["%sql\n","DESCRIBE EXTENDED sales_csv"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<center><img src=\"https://i.postimg.cc/j2M6393G/db350.png\"></center>"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"74efa6cd-5530-4ac2-8c7e-960cdc6d375b","showTitle":false,"title":""}},"source":["#### **Límites de Tablas con Fuentes de Datos Externas**\n","\n","Si usted ha tomado otros cursos sobre Databricks o revisado cualquier literatura de nuestra compañía, usted puede haber oído hablar de Delta Lake y Lakehouse. Tenga en cuenta que siempre que estemos definiendo tablas o consultas contra fuentes de datos externas, **no podemos** esperar las garantías de rendimiento asociadas a Delta Lake y Lakehouse.\n","\n","Por ejemplo: mientras que las tablas de Delta Lake garantizarán que siempre se consulte la versión más reciente de los datos de origen, las tablas registradas contra otras fuentes de datos pueden representar versiones anteriores almacenadas en caché.\n","\n","La siguiente celda ejecuta cierta lógica que podemos considerar como la representación de un sistema externo que actualiza directamente los archivos subyacentes a nuestra tabla."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"88b13fc5-f272-4266-9906-9e4a1c174598","showTitle":false,"title":""}},"outputs":[],"source":["%python\n","(spark.read\n","      .option(\"header\", \"true\")\n","      .option(\"delimiter\", \"|\")\n","      .csv(DA.paths.sales_csv)\n","      .write.mode(\"append\")\n","      .format(\"csv\")\n","      .save(DA.paths.sales_csv))"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"c3fc87ff-2267-415f-b5b1-662cd67dc40f","showTitle":false,"title":""}},"source":["Si miramos el recuento actual de registros en nuestra tabla, el número que veremos no reflejará estas filas recién insertadas."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"2bbdbc65-2266-4233-a381-4d0293985cec","showTitle":false,"title":""}},"outputs":[],"source":["%sql\n","SELECT COUNT(*) FROM sales_csv"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<center><img src=\"https://i.postimg.cc/0Qd0DrY5/db351.png\"></center>"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"415773a6-9a60-48be-9de6-73200c089174","showTitle":false,"title":""}},"source":["En el momento en que consultamos previamente esta fuente de datos, Spark almacenó automáticamente en caché los datos subyacentes en el almacenamiento local. Esto asegura que en consultas posteriores, Spark proporcionará el rendimiento óptimo simplemente consultando esta caché local.\n","\n","Nuestra fuente de datos externa no está configurada para indicar a Spark que debe actualizar estos datos. \n","\n","**Podemos** refrescar manualmente la caché de nuestros datos ejecutando el comando **`REFRESH TABLE`**."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"1e655829-211d-40dd-b055-467344571405","showTitle":false,"title":""}},"outputs":[],"source":["%sql\n","REFRESH TABLE sales_csv"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"24f27626-e1ad-4dcd-8ff7-f8a2fe3045f0","showTitle":false,"title":""}},"source":["Tenga en cuenta que la actualización de la tabla invalidará la caché, lo que significa que tendremos que volver a escanear la fuente de datos original y recuperar todos los datos de la memoria. \n","\n","En el caso de conjuntos de datos muy grandes, esto puede llevar bastante tiempo."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"a0102b27-5aa6-405c-86cc-8e9831947c71","showTitle":false,"title":""}},"outputs":[],"source":["%sql\n","SELECT COUNT(*) FROM sales_csv"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<center><img src=\"https://i.postimg.cc/W4Znp5tk/db352.png\"></center>"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"d470f192-097c-4ed8-bb05-b27b2f786e60","showTitle":false,"title":""}},"source":["#### **Extracción de datos de bases de datos SQL**\n","Las bases de datos SQL son una fuente de datos extremadamente común, y Databricks tiene un driver JDBC estándar para conectarse con muchos tipos de SQL.\n","\n","La sintaxis general para crear estas conexiones es:\n","\n","<strong><code>\n","CREATE TABLE <jdbcTable><br/>\n","USING JDBC<br/>\n","OPTIONS (<br/>\n","&nbsp; &nbsp; url = \"jdbc:{databaseServerType}://{jdbcHostname}:{jdbcPort}\",<br/>\n","&nbsp; &nbsp; dbtable = \"{jdbcDatabase}.table\",<br/>\n","&nbsp; &nbsp; user = \"{jdbcUsername}\",<br/>\n","&nbsp; &nbsp; password = \"{jdbcPassword}\"<br/>\n",")\n","</code></strong>\n","\n","En el siguiente ejemplo de código, nos conectaremos con <a href=\"https://www.sqlite.org/index.html\" target=\"_blank\">SQLite</a>.\n","  \n","**NOTA:** SQLite utiliza un archivo local para almacenar una base de datos, y no requiere un puerto, nombre de usuario o contraseña.\n","  \n","<img src=\"https://files.training.databricks.com/images/icon_warn_24.png\"> **AVISO**: La configuración backend del servidor JDBC asume que está ejecutando este notebook en un single-node cluster. Si está ejecutando en un clúster con múltiples workers, el cliente que se ejecuta en los executors no será capaz de conectarse al driver."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"ce9b123a-e88c-4b55-b50e-e6d59aeeadce","showTitle":false,"title":""}},"outputs":[],"source":["%sql\n","DROP TABLE IF EXISTS users_jdbc;\n","\n","CREATE TABLE users_jdbc\n","USING JDBC\n","OPTIONS (\n","  url = \"jdbc:sqlite:${DA.paths.ecommerce_db}\",\n","  dbtable = \"users\"\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"bd7e7f0e-4f68-483d-af09-a4fd793de91e","showTitle":false,"title":""}},"source":["Ahora podemos consultar esta tabla como si estuviera definida localmente."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"6971fb24-3bed-4100-aeba-49eda0a6163e","showTitle":false,"title":""}},"outputs":[],"source":["%sql\n","SELECT * FROM users_jdbc"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<center><img src=\"https://i.postimg.cc/zfTCY3Pw/db353.png\"></center>"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"ae5cd395-d346-42a5-90c5-c0a497946576","showTitle":false,"title":""}},"source":["Si observamos los metadatos de la tabla, veremos que hemos capturado la información del schema del sistema externo.\n","\n","Las propiedades de almacenamiento (que incluirían el nombre de usuario y la contraseña asociados a la conexión) se redactan automáticamente."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"fa54e16a-7c85-4a18-84ed-926cb2875e55","showTitle":false,"title":""}},"outputs":[],"source":["%sql\n","DESCRIBE EXTENDED users_jdbc"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<center><img src=\"https://i.postimg.cc/G3Kjp83q/db354.png\"></center>"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"228730d0-c2ae-4eef-ac20-14e3fa1337df","showTitle":false,"title":""}},"source":["Mientras que la tabla aparece como **`MANAGED`**, el listado del contenido de la ubicación especificada confirma que no se está persistiendo ningún dato localmente."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"41766c43-5af8-402e-94b4-15aaf8caf936","showTitle":false,"title":""}},"outputs":[],"source":["%python\n","import pyspark.sql.functions as F\n","\n","location = spark.sql(\"DESCRIBE EXTENDED users_jdbc\").filter(F.col(\"col_name\") == \"Location\").first()[\"data_type\"]\n","print(location)\n","\n","files = dbutils.fs.ls(location)\n","print(f\"Found {len(files)} files\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<center><img src=\"https://i.postimg.cc/L8ctWjv6/db355.png\"></center>"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"57369a08-b526-43a2-9e5d-b497cb971403","showTitle":false,"title":""}},"source":["Tenga en cuenta que algunos sistemas SQL, como los Data Warehouse, tendrán drivers personalizados. Spark interactuará con varias bases de datos externas de manera diferente, pero los dos enfoques básicos se pueden resumir en:\n","1. Trasladar toda la tabla o tablas de origen a Databricks y luego ejecutar la lógica en el clúster actualmente activo.\n","1. Empujar la consulta a la base de datos SQL externa y sólo transferir los resultados de vuelta a Databricks\n","\n","En cualquiera de los casos, trabajar con conjuntos de datos muy grandes en bases de datos SQL externas puede incurrir en una sobrecarga significativa debido a:\n","1. Latencia de transferencia de red asociada con el movimiento de todos los datos a través de la Internet pública\n","1. Ejecución de la lógica de consulta en sistemas de origen no optimizados para consultas de big data"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"63f786c7-b935-4d8a-a759-f5c2f541bdc0","showTitle":false,"title":""}},"source":["Ejecute la siguiente celda para eliminar las tablas y archivos asociados a esta lección."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"0c192277-eca8-4504-a144-1071ccdad45b","showTitle":false,"title":""}},"outputs":[],"source":["%python\n","DA.cleanup()"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"ace6fe79-370f-49b9-9fea-0a623419e610","showTitle":false,"title":""}},"source":["-sandbox\n","&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n","Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n","<br/>\n","<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":2},"notebookName":"DE 2.2 - Providing Options for External Sources","widgets":{}},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
