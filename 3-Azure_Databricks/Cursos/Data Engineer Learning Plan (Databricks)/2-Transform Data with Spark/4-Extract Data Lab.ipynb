{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"58334330-5ac9-476a-a0f3-f5ed7550f424","showTitle":false,"title":""}},"source":["<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n","  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n","</div>"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"8d2e8132-5a7f-4a3a-a6ea-bcdcdb8aa135","showTitle":false,"title":""}},"source":["### **Laboratorio de extracción de datos**\n","\n","En este laboratorio, extraerá datos sin procesar de archivos JSON.\n","\n","#### **Objetivos de aprendizaje**\n","Al finalizar este laboratorio, deberá ser capaz de\n","- Registrar una tabla externa para extraer datos de archivos JSON"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"e2f86f57-1764-455b-868d-1499e8aad763","showTitle":false,"title":""}},"source":["#### **Run Setup**\n","\n","Ejecute la siguiente celda para configurar variables y conjuntos de datos para esta lección."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"3cf907e1-b63d-4f20-891c-4bd205b24a54","showTitle":false,"title":""}},"outputs":[],"source":["%run ../Includes/Classroom-Setup-02.3L"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"52d375e0-5f1c-498e-87c6-dcdd674bcd41","showTitle":false,"title":""}},"source":["#### **Visión general de los datos**\n","\n","Trabajaremos con una muestra de datos Kafka en bruto escritos como archivos JSON. \n","\n","Cada archivo contiene todos los registros consumidos durante un intervalo de 5 segundos, almacenados con el schema completo de Kafka como un archivo JSON de múltiples registros. \n","\n","El schema para la tabla:\n","\n","| field  | type | description |\n","| ------ | ---- | ----------- |\n","| key    | BINARY | The **`user_id`** field is used as the key; this is a unique alphanumeric field that corresponds to session/cookie information |\n","| offset | LONG | This is a unique value, monotonically increasing for each partition |\n","| partition | INTEGER | Our current Kafka implementation uses only 2 partitions (0 and 1) |\n","| timestamp | LONG    | This timestamp is recorded as milliseconds since epoch, and represents the time at which the producer appends a record to a partition |\n","| topic | STRING | While the Kafka service hosts multiple topics, only those records from the **`clickstream`** topic are included here |\n","| value | BINARY | This is the full data payload (to be discussed later), sent as JSON |"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"89d21264-4338-4d7e-b3f7-e3a35ae719b2","showTitle":false,"title":""}},"source":["#### **Extraer eventos sin procesar (raw events) de archivos JSON**\n","Para cargar estos datos en Delta correctamente, primero tenemos que extraer los datos JSON utilizando el schema correcto.\n","\n","Cree una tabla externa con los archivos JSON ubicados en la ruta de archivo que se indica a continuación. Nombre esta tabla **`events_json`** y declare el schema anterior."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"d001dabe-81d2-4eea-8e05-cd65da3cbdae","showTitle":false,"title":""}},"outputs":[],"source":["%sql\n","CREATE TABLA IF NOT EXISTS events_json (\n","  key BINARY,\n","  offset BIGINT,\n","  partition INT,\n","  timestamp BIGINT,\n","  topic STRING,\n","  value BINARY)\n","USING JSON\n","LOCATION \"${DA.paths.kafka_events}\" "]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"910d1eb3-57c7-4204-ae4a-b092163f6f85","showTitle":false,"title":""}},"source":["**NOTA**: Usaremos Python para realizar comprobaciones ocasionalmente a lo largo del laboratorio. La siguiente celda devolverá un error con un mensaje sobre lo que hay que cambiar si no has seguido las instrucciones. Si no hay salida de la ejecución de la celda significa que has completado este paso."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"cc7171de-f154-4875-8f03-4ac23d691162","showTitle":false,"title":""}},"outputs":[],"source":["%python\n","assert spark.table(\"events_json\"), \"Table named `events_json` does not exist\"\n","assert spark.table(\"events_json\").columns == ['key', 'offset', 'partition', 'timestamp', 'topic', 'value'], \"Please name the columns in the order provided above\"\n","assert spark.table(\"events_json\").dtypes == [('key', 'binary'), ('offset', 'bigint'), ('partition', 'int'), ('timestamp', 'bigint'), ('topic', 'string'), ('value', 'binary')], \"Please make sure the column types are identical to those provided above\"\n","\n","total = spark.table(\"events_json\").count()\n","assert total == 2252, f\"Expected 2252 records, found {total}\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%sql\n","SELECT * FROM events_json"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<center><img src=\"https://i.postimg.cc/0jfNx1py/db356.png\"></center>"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"8b7e3cd0-b030-46d9-9080-6f6af68cae82","showTitle":false,"title":""}},"source":["Ejecute la siguiente celda para eliminar las tablas y archivos asociados a esta lección."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"2cedb468-64ca-457f-8f33-0b64fe6ddd45","showTitle":false,"title":""}},"outputs":[],"source":["%python\n","DA.cleanup()"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"a66cabb7-4e8d-4214-b5c7-8878043a721b","showTitle":false,"title":""}},"source":["-sandbox\n","&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n","Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n","<br/>\n","<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":2},"notebookName":"DE 2.3L - Extract Data Lab","widgets":{}},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
